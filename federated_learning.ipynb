{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZeI-oWBm7pb",
        "outputId": "e09ec2e8-fd4b-47af-b988-0eecfe7e4192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(('/content/drive'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "68RvyCywm1xi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import glob\n",
        "import copy\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NvGn8v9xm2OU"
      },
      "outputs": [],
      "source": [
        "# Two-hidden-layer MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim_in, dim_hidden1, dim_hidden2, dim_out):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim_in, dim_hidden1)\n",
        "        self.bn1 = nn.BatchNorm1d(dim_hidden1)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(dim_hidden1, dim_hidden2)\n",
        "        self.bn2 = nn.BatchNorm1d(dim_hidden2)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(dim_hidden2, dim_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return torch.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hPZ1lFvgm5io"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data quality check and preparation function\n",
        "def prepare_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan).dropna() # Handling Inf and -Inf\n",
        "    df['marker_encoded'] = df['marker'].map({'Attack': 1, 'Natural': 0})\n",
        "    df = df.drop(columns=['marker']).dropna()\n",
        "    X = df.drop(columns=['marker_encoded']).values\n",
        "    y = df['marker_encoded'].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, y"
      ],
      "metadata": {
        "id": "6wfa8ceNQt2q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to average the weights of models\n",
        "def average_weights(model_weights):\n",
        "    global_weights = copy.deepcopy(model_weights[0])\n",
        "    for key in global_weights.keys():\n",
        "        for i in range(1, len(model_weights)):\n",
        "            global_weights[key] += model_weights[i][key]\n",
        "        global_weights[key] = torch.div(global_weights[key], len(model_weights))\n",
        "    return global_weights"
      ],
      "metadata": {
        "id": "swN_ycGmQxFg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def federated_training(models, datasets, epochs, l1_lambda=0.001, l2_lambda=0.001):\n",
        "    global_model = models[0]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "\n",
        "    for idx, (model, dataset) in enumerate(zip(models, datasets)):\n",
        "        best_loss = np.inf\n",
        "        no_improve_epoch = 0\n",
        "        early_stopping_thresh = 10  # Early stopping threshold\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=l2_lambda)  # L2 regularization\n",
        "        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "        print(f\"Training model for Dataset {idx + 1}/{len(datasets)}\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            local_loss = 0\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "            for data, target in loader:\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                l1_regularization = sum(param.abs().sum() for param in model.parameters())  # L1 regularization\n",
        "                total_loss = loss + l1_lambda * l1_regularization\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "                local_loss += total_loss.item()\n",
        "\n",
        "            # Early stopping\n",
        "            scheduler.step(local_loss)\n",
        "            if local_loss < best_loss:\n",
        "                best_loss = local_loss\n",
        "                no_improve_epoch = 0\n",
        "            else:\n",
        "                no_improve_epoch += 1\n",
        "                if no_improve_epoch >= early_stopping_thresh:\n",
        "                    print(f\"Early stopping triggered for Dataset {idx + 1} model after epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "            print(f'Dataset {idx + 1} model training, Epoch {epoch+1}, Loss: {local_loss / len(loader)}')\n",
        "\n",
        "        print(50*\"-\")\n",
        "\n",
        "        # Updating the global model after each model is trained for 'epochs' or early stopped\n",
        "        global_model.load_state_dict(model.state_dict(), strict=False)\n",
        "\n",
        "    local_weights = [model.state_dict() for model in models]\n",
        "    global_weights = average_weights(local_weights)\n",
        "    global_model.load_state_dict(global_weights)\n",
        "\n",
        "    return global_model"
      ],
      "metadata": {
        "id": "J3cp7EpSQxtX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_dataset):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        loader = DataLoader(test_dataset, batch_size=32)\n",
        "        for data, target in loader:\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total += target.size(0)\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Test set: Accuracy: {accuracy}%')"
      ],
      "metadata": {
        "id": "pbyWUYMjQ_5s"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading datasets\n",
        "folder_path = '/content/drive/MyDrive/GRA/Project/dataset'\n",
        "file_paths = glob.glob(os.path.join(folder_path, '*.csv'))\n",
        "datasets = [CustomDataset(*prepare_data(file)) for file in file_paths]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-ab0buWRAwI",
        "outputId": "111bafdd-52d3-44ce-c166-6f577950eaca"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-514d72e09884>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['marker_encoded'] = df['marker'].map({'Attack': 1, 'Natural': 0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing test dataset\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(*prepare_data(file_paths[14]), test_size=0.2)\n",
        "# test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    all_features.extend(dataset.features.numpy())\n",
        "    all_labels.extend(dataset.labels.numpy())\n",
        "\n",
        "all_features = np.array(all_features)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "test_dataset = CustomDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "uoePxme7RGQb"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = X_train.shape[1]\n",
        "num_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94Yy5b5URpuW",
        "outputId": "85bcd1b3-158f-4892-bd36-07c76a41f1ae"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing models for each dataset\n",
        "models = [MLP(dim_in=num_features, dim_hidden1=100, dim_hidden2=50, dim_out=2) for _ in datasets]"
      ],
      "metadata": {
        "id": "wqPTnjaMRroU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the models in a federated way with aggregation\n",
        "global_model = federated_training(models, datasets, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG0UMR6RRvqC",
        "outputId": "2f43733a-eb91-4d09-9c42-a7877705cd0e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for Dataset 1/15\n",
            "Dataset 1 model training, Epoch 1, Loss: 1.3224870327271914\n",
            "Dataset 1 model training, Epoch 2, Loss: 1.0595907180717117\n",
            "Dataset 1 model training, Epoch 3, Loss: 0.9137440327751009\n",
            "Dataset 1 model training, Epoch 4, Loss: 0.8310335505949823\n",
            "Dataset 1 model training, Epoch 5, Loss: 0.7868308358286557\n",
            "Dataset 1 model training, Epoch 6, Loss: 0.7576124695570845\n",
            "Dataset 1 model training, Epoch 7, Loss: 0.7365399563783094\n",
            "Dataset 1 model training, Epoch 8, Loss: 0.7189139908081607\n",
            "Dataset 1 model training, Epoch 9, Loss: 0.7021303506273973\n",
            "Dataset 1 model training, Epoch 10, Loss: 0.6827802148304487\n",
            "Dataset 1 model training, Epoch 11, Loss: 0.667420935277876\n",
            "Dataset 1 model training, Epoch 12, Loss: 0.6578408524786171\n",
            "Dataset 1 model training, Epoch 13, Loss: 0.643944878523287\n",
            "Dataset 1 model training, Epoch 14, Loss: 0.626386142482883\n",
            "Dataset 1 model training, Epoch 15, Loss: 0.6173758951849059\n",
            "Dataset 1 model training, Epoch 16, Loss: 0.6081043891608715\n",
            "Dataset 1 model training, Epoch 17, Loss: 0.5953480860120371\n",
            "Dataset 1 model training, Epoch 18, Loss: 0.5816766211861059\n",
            "Dataset 1 model training, Epoch 19, Loss: 0.5743308867278852\n",
            "Dataset 1 model training, Epoch 20, Loss: 0.5647689066827297\n",
            "Dataset 1 model training, Epoch 21, Loss: 0.5685930361873225\n",
            "Dataset 1 model training, Epoch 22, Loss: 0.5603937596866959\n",
            "Dataset 1 model training, Epoch 23, Loss: 0.5549245862976501\n",
            "Dataset 1 model training, Epoch 24, Loss: 0.547130921757535\n",
            "Dataset 1 model training, Epoch 25, Loss: 0.5469173200820622\n",
            "Dataset 1 model training, Epoch 26, Loss: 0.5432917138463572\n",
            "Dataset 1 model training, Epoch 27, Loss: 0.5458024571600714\n",
            "Dataset 1 model training, Epoch 28, Loss: 0.54466531190433\n",
            "Dataset 1 model training, Epoch 29, Loss: 0.5346215688868573\n",
            "Dataset 1 model training, Epoch 30, Loss: 0.5363905859228811\n",
            "Dataset 1 model training, Epoch 31, Loss: 0.5349419209125795\n",
            "Dataset 1 model training, Epoch 32, Loss: 0.5306389557295724\n",
            "Dataset 1 model training, Epoch 33, Loss: 0.5347751067265084\n",
            "Dataset 1 model training, Epoch 34, Loss: 0.5295522330622924\n",
            "Dataset 1 model training, Epoch 35, Loss: 0.5297817146699679\n",
            "Dataset 1 model training, Epoch 36, Loss: 0.5301277939426271\n",
            "Dataset 1 model training, Epoch 37, Loss: 0.5302821191910067\n",
            "Dataset 1 model training, Epoch 38, Loss: 0.5233368167751714\n",
            "Dataset 1 model training, Epoch 39, Loss: 0.5305831857809895\n",
            "Dataset 1 model training, Epoch 40, Loss: 0.5230774979450201\n",
            "Dataset 1 model training, Epoch 41, Loss: 0.5264690475244271\n",
            "Dataset 1 model training, Epoch 42, Loss: 0.5206479969385424\n",
            "Dataset 1 model training, Epoch 43, Loss: 0.5239222108533508\n",
            "Dataset 1 model training, Epoch 44, Loss: 0.5193008135416006\n",
            "Dataset 1 model training, Epoch 45, Loss: 0.5245448652850954\n",
            "Dataset 1 model training, Epoch 46, Loss: 0.5216045914904067\n",
            "Dataset 1 model training, Epoch 47, Loss: 0.5177148508006021\n",
            "Dataset 1 model training, Epoch 48, Loss: 0.5147218078766999\n",
            "Dataset 1 model training, Epoch 49, Loss: 0.5226448374359232\n",
            "Dataset 1 model training, Epoch 50, Loss: 0.5222181562138232\n",
            "Dataset 1 model training, Epoch 51, Loss: 0.5123347636116179\n",
            "Dataset 1 model training, Epoch 52, Loss: 0.5134912231250813\n",
            "Dataset 1 model training, Epoch 53, Loss: 0.52310662540166\n",
            "Dataset 1 model training, Epoch 54, Loss: 0.5163708235088148\n",
            "Dataset 1 model training, Epoch 55, Loss: 0.5125091136678269\n",
            "Dataset 1 model training, Epoch 56, Loss: 0.5134460269228408\n",
            "Dataset 1 model training, Epoch 57, Loss: 0.5105199519740907\n",
            "Dataset 1 model training, Epoch 58, Loss: 0.5119805910477513\n",
            "Dataset 1 model training, Epoch 59, Loss: 0.5065645255932683\n",
            "Dataset 1 model training, Epoch 60, Loss: 0.5135832172083227\n",
            "Dataset 1 model training, Epoch 61, Loss: 0.5149729200883916\n",
            "Dataset 1 model training, Epoch 62, Loss: 0.5051732435822487\n",
            "Dataset 1 model training, Epoch 63, Loss: 0.510002755216862\n",
            "Dataset 1 model training, Epoch 64, Loss: 0.509249880517784\n",
            "Dataset 1 model training, Epoch 65, Loss: 0.5103846143342947\n",
            "Dataset 1 model training, Epoch 66, Loss: 0.5011647171095798\n",
            "Dataset 1 model training, Epoch 67, Loss: 0.5041146901877303\n",
            "Dataset 1 model training, Epoch 68, Loss: 0.5122999954772623\n",
            "Dataset 1 model training, Epoch 69, Loss: 0.5114835041918253\n",
            "Dataset 1 model training, Epoch 70, Loss: 0.5067215150124148\n",
            "Dataset 1 model training, Epoch 71, Loss: 0.505588924610301\n",
            "Dataset 1 model training, Epoch 72, Loss: 0.5020053231794583\n",
            "Dataset 1 model training, Epoch 73, Loss: 0.5026689614904555\n",
            "Dataset 1 model training, Epoch 74, Loss: 0.49450472487430824\n",
            "Dataset 1 model training, Epoch 75, Loss: 0.4974144730520876\n",
            "Dataset 1 model training, Epoch 76, Loss: 0.4884141163998528\n",
            "Dataset 1 model training, Epoch 77, Loss: 0.49244879166546623\n",
            "Dataset 1 model training, Epoch 78, Loss: 0.4915574751794338\n",
            "Dataset 1 model training, Epoch 79, Loss: 0.4888007456534787\n",
            "Dataset 1 model training, Epoch 80, Loss: 0.4920488029326263\n",
            "Dataset 1 model training, Epoch 81, Loss: 0.490782174232759\n",
            "Dataset 1 model training, Epoch 82, Loss: 0.48414605718694237\n",
            "Dataset 1 model training, Epoch 83, Loss: 0.4891549490000072\n",
            "Dataset 1 model training, Epoch 84, Loss: 0.49675739850652845\n",
            "Dataset 1 model training, Epoch 85, Loss: 0.4850219266587182\n",
            "Dataset 1 model training, Epoch 86, Loss: 0.48981186414235517\n",
            "Dataset 1 model training, Epoch 87, Loss: 0.4880367786084351\n",
            "Dataset 1 model training, Epoch 88, Loss: 0.4828243171306033\n",
            "Dataset 1 model training, Epoch 89, Loss: 0.4850519311271216\n",
            "Dataset 1 model training, Epoch 90, Loss: 0.4936169207488236\n",
            "Dataset 1 model training, Epoch 91, Loss: 0.48997376251377556\n",
            "Dataset 1 model training, Epoch 92, Loss: 0.48489841524707644\n",
            "Dataset 1 model training, Epoch 93, Loss: 0.4811250108637308\n",
            "Dataset 1 model training, Epoch 94, Loss: 0.48535486722463056\n",
            "Dataset 1 model training, Epoch 95, Loss: 0.48464696521037504\n",
            "Dataset 1 model training, Epoch 96, Loss: 0.4893992509888975\n",
            "Dataset 1 model training, Epoch 97, Loss: 0.4835755452513695\n",
            "Dataset 1 model training, Epoch 98, Loss: 0.4828362631562509\n",
            "Dataset 1 model training, Epoch 99, Loss: 0.4840593138023427\n",
            "Dataset 1 model training, Epoch 100, Loss: 0.4858505841540663\n",
            "Dataset 1 model training, Epoch 101, Loss: 0.48062627821376447\n",
            "Dataset 1 model training, Epoch 102, Loss: 0.4759203844556683\n",
            "Dataset 1 model training, Epoch 103, Loss: 0.47751693682450996\n",
            "Dataset 1 model training, Epoch 104, Loss: 0.4695046498979393\n",
            "Dataset 1 model training, Epoch 105, Loss: 0.4738881958550529\n",
            "Dataset 1 model training, Epoch 106, Loss: 0.47075831929319784\n",
            "Dataset 1 model training, Epoch 107, Loss: 0.47725444796838257\n",
            "Dataset 1 model training, Epoch 108, Loss: 0.4747876905296978\n",
            "Dataset 1 model training, Epoch 109, Loss: 0.4717899319764815\n",
            "Dataset 1 model training, Epoch 110, Loss: 0.469301356884994\n",
            "Dataset 1 model training, Epoch 111, Loss: 0.4685938462222877\n",
            "Dataset 1 model training, Epoch 112, Loss: 0.47039539112072243\n",
            "Dataset 1 model training, Epoch 113, Loss: 0.47202152503948464\n",
            "Dataset 1 model training, Epoch 114, Loss: 0.4679822421779758\n",
            "Dataset 1 model training, Epoch 115, Loss: 0.4686414505306043\n",
            "Dataset 1 model training, Epoch 116, Loss: 0.4635630120572291\n",
            "Dataset 1 model training, Epoch 117, Loss: 0.4675983657178126\n",
            "Dataset 1 model training, Epoch 118, Loss: 0.467372807625093\n",
            "Dataset 1 model training, Epoch 119, Loss: 0.47058125291215747\n",
            "Dataset 1 model training, Epoch 120, Loss: 0.4715010735549425\n",
            "Dataset 1 model training, Epoch 121, Loss: 0.4671586751937866\n",
            "Dataset 1 model training, Epoch 122, Loss: 0.4707071155701813\n",
            "Dataset 1 model training, Epoch 123, Loss: 0.4573625965338004\n",
            "Dataset 1 model training, Epoch 124, Loss: 0.461599949747324\n",
            "Dataset 1 model training, Epoch 125, Loss: 0.4643365123162144\n",
            "Dataset 1 model training, Epoch 126, Loss: 0.45875282291519015\n",
            "Dataset 1 model training, Epoch 127, Loss: 0.4647371557198073\n",
            "Dataset 1 model training, Epoch 128, Loss: 0.4669281052131402\n",
            "Dataset 1 model training, Epoch 129, Loss: 0.4591775040485357\n",
            "Dataset 1 model training, Epoch 130, Loss: 0.4632297158241272\n",
            "Dataset 1 model training, Epoch 131, Loss: 0.45816397510076823\n",
            "Dataset 1 model training, Epoch 132, Loss: 0.4586839670021283\n",
            "Dataset 1 model training, Epoch 133, Loss: 0.453735907611094\n",
            "Dataset 1 model training, Epoch 134, Loss: 0.4530286757569564\n",
            "Dataset 1 model training, Epoch 135, Loss: 0.45547595443694217\n",
            "Dataset 1 model training, Epoch 136, Loss: 0.4560017317141357\n",
            "Dataset 1 model training, Epoch 137, Loss: 0.45667644139183194\n",
            "Dataset 1 model training, Epoch 138, Loss: 0.4567595357565503\n",
            "Dataset 1 model training, Epoch 139, Loss: 0.45341045076125547\n",
            "Dataset 1 model training, Epoch 140, Loss: 0.4580306728419505\n",
            "Dataset 1 model training, Epoch 141, Loss: 0.45449339049427134\n",
            "Dataset 1 model training, Epoch 142, Loss: 0.45445701636766134\n",
            "Dataset 1 model training, Epoch 143, Loss: 0.454436329830634\n",
            "Early stopping triggered for Dataset 1 model after epoch 144\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 2/15\n",
            "Dataset 2 model training, Epoch 1, Loss: 1.4043380844977593\n",
            "Dataset 2 model training, Epoch 2, Loss: 1.1554492669720804\n",
            "Dataset 2 model training, Epoch 3, Loss: 1.005208194640375\n",
            "Dataset 2 model training, Epoch 4, Loss: 0.9066820086971406\n",
            "Dataset 2 model training, Epoch 5, Loss: 0.8549440999184885\n",
            "Dataset 2 model training, Epoch 6, Loss: 0.822636785814839\n",
            "Dataset 2 model training, Epoch 7, Loss: 0.8038198336478203\n",
            "Dataset 2 model training, Epoch 8, Loss: 0.7811468074398656\n",
            "Dataset 2 model training, Epoch 9, Loss: 0.765334197398155\n",
            "Dataset 2 model training, Epoch 10, Loss: 0.7494154718614394\n",
            "Dataset 2 model training, Epoch 11, Loss: 0.7361247143437786\n",
            "Dataset 2 model training, Epoch 12, Loss: 0.724156649651066\n",
            "Dataset 2 model training, Epoch 13, Loss: 0.7174030919228831\n",
            "Dataset 2 model training, Epoch 14, Loss: 0.7024615706936006\n",
            "Dataset 2 model training, Epoch 15, Loss: 0.6878930411031169\n",
            "Dataset 2 model training, Epoch 16, Loss: 0.6751295312758415\n",
            "Dataset 2 model training, Epoch 17, Loss: 0.6702301979064942\n",
            "Dataset 2 model training, Epoch 18, Loss: 0.6629833036853421\n",
            "Dataset 2 model training, Epoch 19, Loss: 0.6593554885156693\n",
            "Dataset 2 model training, Epoch 20, Loss: 0.6446562024854845\n",
            "Dataset 2 model training, Epoch 21, Loss: 0.6435483259539451\n",
            "Dataset 2 model training, Epoch 22, Loss: 0.6395873813859878\n",
            "Dataset 2 model training, Epoch 23, Loss: 0.634970058164289\n",
            "Dataset 2 model training, Epoch 24, Loss: 0.6392797616220289\n",
            "Dataset 2 model training, Epoch 25, Loss: 0.634194843422982\n",
            "Dataset 2 model training, Epoch 26, Loss: 0.6341173110469696\n",
            "Dataset 2 model training, Epoch 27, Loss: 0.6263857743432445\n",
            "Dataset 2 model training, Epoch 28, Loss: 0.6271529134242765\n",
            "Dataset 2 model training, Epoch 29, Loss: 0.6310302063342064\n",
            "Dataset 2 model training, Epoch 30, Loss: 0.6267070524154171\n",
            "Dataset 2 model training, Epoch 31, Loss: 0.6203612789030998\n",
            "Dataset 2 model training, Epoch 32, Loss: 0.6191505947420674\n",
            "Dataset 2 model training, Epoch 33, Loss: 0.6266809353905339\n",
            "Dataset 2 model training, Epoch 34, Loss: 0.617424738599408\n",
            "Dataset 2 model training, Epoch 35, Loss: 0.620557362802567\n",
            "Dataset 2 model training, Epoch 36, Loss: 0.6177128249599088\n",
            "Dataset 2 model training, Epoch 37, Loss: 0.6141049431216332\n",
            "Dataset 2 model training, Epoch 38, Loss: 0.6105984555136773\n",
            "Dataset 2 model training, Epoch 39, Loss: 0.6167492828061504\n",
            "Dataset 2 model training, Epoch 40, Loss: 0.619136449598497\n",
            "Dataset 2 model training, Epoch 41, Loss: 0.6089823707457511\n",
            "Dataset 2 model training, Epoch 42, Loss: 0.6137976311868237\n",
            "Dataset 2 model training, Epoch 43, Loss: 0.6072322212880658\n",
            "Dataset 2 model training, Epoch 44, Loss: 0.6046958984867219\n",
            "Dataset 2 model training, Epoch 45, Loss: 0.6096118179059797\n",
            "Dataset 2 model training, Epoch 46, Loss: 0.6024183734770744\n",
            "Dataset 2 model training, Epoch 47, Loss: 0.6071799764710087\n",
            "Dataset 2 model training, Epoch 48, Loss: 0.6121451583600813\n",
            "Dataset 2 model training, Epoch 49, Loss: 0.6061638735955761\n",
            "Dataset 2 model training, Epoch 50, Loss: 0.6037752561030849\n",
            "Dataset 2 model training, Epoch 51, Loss: 0.6004871731804263\n",
            "Dataset 2 model training, Epoch 52, Loss: 0.6049215284086042\n",
            "Dataset 2 model training, Epoch 53, Loss: 0.603684877580212\n",
            "Dataset 2 model training, Epoch 54, Loss: 0.6070736837002539\n",
            "Dataset 2 model training, Epoch 55, Loss: 0.6061859228918629\n",
            "Dataset 2 model training, Epoch 56, Loss: 0.6015595884092393\n",
            "Dataset 2 model training, Epoch 57, Loss: 0.6050421566732468\n",
            "Dataset 2 model training, Epoch 58, Loss: 0.5986041517026963\n",
            "Dataset 2 model training, Epoch 59, Loss: 0.5904951210944883\n",
            "Dataset 2 model training, Epoch 60, Loss: 0.5832703511561117\n",
            "Dataset 2 model training, Epoch 61, Loss: 0.5832782893411574\n",
            "Dataset 2 model training, Epoch 62, Loss: 0.5864416703101127\n",
            "Dataset 2 model training, Epoch 63, Loss: 0.5887309845416776\n",
            "Dataset 2 model training, Epoch 64, Loss: 0.5857507221160396\n",
            "Dataset 2 model training, Epoch 65, Loss: 0.5790199345157993\n",
            "Dataset 2 model training, Epoch 66, Loss: 0.5913652889190182\n",
            "Dataset 2 model training, Epoch 67, Loss: 0.5796765627399567\n",
            "Dataset 2 model training, Epoch 68, Loss: 0.5766898497458427\n",
            "Dataset 2 model training, Epoch 69, Loss: 0.586193153742821\n",
            "Dataset 2 model training, Epoch 70, Loss: 0.5828862942034199\n",
            "Dataset 2 model training, Epoch 71, Loss: 0.5796615896686431\n",
            "Dataset 2 model training, Epoch 72, Loss: 0.5804939454601658\n",
            "Dataset 2 model training, Epoch 73, Loss: 0.5767505682283832\n",
            "Dataset 2 model training, Epoch 74, Loss: 0.5809104844447105\n",
            "Dataset 2 model training, Epoch 75, Loss: 0.5727633999240014\n",
            "Dataset 2 model training, Epoch 76, Loss: 0.5718350389311391\n",
            "Dataset 2 model training, Epoch 77, Loss: 0.5611558637311381\n",
            "Dataset 2 model training, Epoch 78, Loss: 0.5622028993022057\n",
            "Dataset 2 model training, Epoch 79, Loss: 0.565102514143913\n",
            "Dataset 2 model training, Epoch 80, Loss: 0.5669138191207763\n",
            "Dataset 2 model training, Epoch 81, Loss: 0.5623202316222652\n",
            "Dataset 2 model training, Epoch 82, Loss: 0.5590106606483459\n",
            "Dataset 2 model training, Epoch 83, Loss: 0.5606418298136804\n",
            "Dataset 2 model training, Epoch 84, Loss: 0.5561195440830723\n",
            "Dataset 2 model training, Epoch 85, Loss: 0.5567503790701589\n",
            "Dataset 2 model training, Epoch 86, Loss: 0.5609864457961051\n",
            "Dataset 2 model training, Epoch 87, Loss: 0.556779166190855\n",
            "Dataset 2 model training, Epoch 88, Loss: 0.5476081890444602\n",
            "Dataset 2 model training, Epoch 89, Loss: 0.5563314466707168\n",
            "Dataset 2 model training, Epoch 90, Loss: 0.5521644580748774\n",
            "Dataset 2 model training, Epoch 91, Loss: 0.5568807949942928\n",
            "Dataset 2 model training, Epoch 92, Loss: 0.5460906367148123\n",
            "Dataset 2 model training, Epoch 93, Loss: 0.55428099305399\n",
            "Dataset 2 model training, Epoch 94, Loss: 0.5537265310364384\n",
            "Dataset 2 model training, Epoch 95, Loss: 0.5480008213750778\n",
            "Dataset 2 model training, Epoch 96, Loss: 0.5500609165237796\n",
            "Dataset 2 model training, Epoch 97, Loss: 0.5513542323343216\n",
            "Dataset 2 model training, Epoch 98, Loss: 0.5513469588372015\n",
            "Dataset 2 model training, Epoch 99, Loss: 0.548371168105833\n",
            "Dataset 2 model training, Epoch 100, Loss: 0.5400872453566521\n",
            "Dataset 2 model training, Epoch 101, Loss: 0.5427282839052139\n",
            "Dataset 2 model training, Epoch 102, Loss: 0.5461746008165421\n",
            "Dataset 2 model training, Epoch 103, Loss: 0.5465061618435767\n",
            "Dataset 2 model training, Epoch 104, Loss: 0.543677589201158\n",
            "Dataset 2 model training, Epoch 105, Loss: 0.54541856723447\n",
            "Dataset 2 model training, Epoch 106, Loss: 0.5384107078275373\n",
            "Dataset 2 model training, Epoch 107, Loss: 0.5425111928293782\n",
            "Dataset 2 model training, Epoch 108, Loss: 0.5395119067161314\n",
            "Dataset 2 model training, Epoch 109, Loss: 0.540793639421463\n",
            "Dataset 2 model training, Epoch 110, Loss: 0.532644719846787\n",
            "Dataset 2 model training, Epoch 111, Loss: 0.545172276035432\n",
            "Dataset 2 model training, Epoch 112, Loss: 0.5454704971082749\n",
            "Dataset 2 model training, Epoch 113, Loss: 0.5379537284374237\n",
            "Dataset 2 model training, Epoch 114, Loss: 0.5402775085741474\n",
            "Dataset 2 model training, Epoch 115, Loss: 0.5373307931807734\n",
            "Dataset 2 model training, Epoch 116, Loss: 0.537615664351371\n",
            "Dataset 2 model training, Epoch 117, Loss: 0.53436920277534\n",
            "Dataset 2 model training, Epoch 118, Loss: 0.5310792905669058\n",
            "Dataset 2 model training, Epoch 119, Loss: 0.5371735801619868\n",
            "Dataset 2 model training, Epoch 120, Loss: 0.5270900534045312\n",
            "Dataset 2 model training, Epoch 121, Loss: 0.5351787828630017\n",
            "Dataset 2 model training, Epoch 122, Loss: 0.5280447534976467\n",
            "Dataset 2 model training, Epoch 123, Loss: 0.5349898636341095\n",
            "Dataset 2 model training, Epoch 124, Loss: 0.533413476520969\n",
            "Dataset 2 model training, Epoch 125, Loss: 0.5318662241581947\n",
            "Dataset 2 model training, Epoch 126, Loss: 0.5312718837491928\n",
            "Dataset 2 model training, Epoch 127, Loss: 0.5303041196638538\n",
            "Dataset 2 model training, Epoch 128, Loss: 0.529299363974602\n",
            "Dataset 2 model training, Epoch 129, Loss: 0.5277222681430078\n",
            "Dataset 2 model training, Epoch 130, Loss: 0.5262978192298643\n",
            "Dataset 2 model training, Epoch 131, Loss: 0.5217565940272424\n",
            "Dataset 2 model training, Epoch 132, Loss: 0.5262956067439049\n",
            "Dataset 2 model training, Epoch 133, Loss: 0.5275341516540897\n",
            "Dataset 2 model training, Epoch 134, Loss: 0.5214967383492377\n",
            "Dataset 2 model training, Epoch 135, Loss: 0.5263841836683212\n",
            "Dataset 2 model training, Epoch 136, Loss: 0.5240200640693787\n",
            "Dataset 2 model training, Epoch 137, Loss: 0.5254788623702141\n",
            "Dataset 2 model training, Epoch 138, Loss: 0.5259877135676723\n",
            "Dataset 2 model training, Epoch 139, Loss: 0.533553958131421\n",
            "Dataset 2 model training, Epoch 140, Loss: 0.5317218753599351\n",
            "Dataset 2 model training, Epoch 141, Loss: 0.523419987963092\n",
            "Dataset 2 model training, Epoch 142, Loss: 0.5174113237088727\n",
            "Dataset 2 model training, Epoch 143, Loss: 0.5219247023905477\n",
            "Dataset 2 model training, Epoch 144, Loss: 0.5229500835941684\n",
            "Dataset 2 model training, Epoch 145, Loss: 0.5234794147552982\n",
            "Dataset 2 model training, Epoch 146, Loss: 0.523082721617914\n",
            "Dataset 2 model training, Epoch 147, Loss: 0.5240723327282937\n",
            "Dataset 2 model training, Epoch 148, Loss: 0.5260748226796427\n",
            "Dataset 2 model training, Epoch 149, Loss: 0.5260275033212477\n",
            "Dataset 2 model training, Epoch 150, Loss: 0.5221647281800547\n",
            "Dataset 2 model training, Epoch 151, Loss: 0.5236350249859595\n",
            "Early stopping triggered for Dataset 2 model after epoch 152\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 3/15\n",
            "Dataset 3 model training, Epoch 1, Loss: 1.3480885457992553\n",
            "Dataset 3 model training, Epoch 2, Loss: 1.087471255461375\n",
            "Dataset 3 model training, Epoch 3, Loss: 0.9440425296624502\n",
            "Dataset 3 model training, Epoch 4, Loss: 0.8644514222939809\n",
            "Dataset 3 model training, Epoch 5, Loss: 0.816881187359492\n",
            "Dataset 3 model training, Epoch 6, Loss: 0.7834567856788636\n",
            "Dataset 3 model training, Epoch 7, Loss: 0.765851772626241\n",
            "Dataset 3 model training, Epoch 8, Loss: 0.7405189216136933\n",
            "Dataset 3 model training, Epoch 9, Loss: 0.7281627798080444\n",
            "Dataset 3 model training, Epoch 10, Loss: 0.7149019396305084\n",
            "Dataset 3 model training, Epoch 11, Loss: 0.6973493878046672\n",
            "Dataset 3 model training, Epoch 12, Loss: 0.6889082252979278\n",
            "Dataset 3 model training, Epoch 13, Loss: 0.6826041988531748\n",
            "Dataset 3 model training, Epoch 14, Loss: 0.6696527828772862\n",
            "Dataset 3 model training, Epoch 15, Loss: 0.6603704176346461\n",
            "Dataset 3 model training, Epoch 16, Loss: 0.6452855815490087\n",
            "Dataset 3 model training, Epoch 17, Loss: 0.6271640672286352\n",
            "Dataset 3 model training, Epoch 18, Loss: 0.6296458578109742\n",
            "Dataset 3 model training, Epoch 19, Loss: 0.6200264807542165\n",
            "Dataset 3 model training, Epoch 20, Loss: 0.6106114806731542\n",
            "Dataset 3 model training, Epoch 21, Loss: 0.603787780602773\n",
            "Dataset 3 model training, Epoch 22, Loss: 0.6035187816619874\n",
            "Dataset 3 model training, Epoch 23, Loss: 0.5966178754965464\n",
            "Dataset 3 model training, Epoch 24, Loss: 0.596687266031901\n",
            "Dataset 3 model training, Epoch 25, Loss: 0.5915821276108424\n",
            "Dataset 3 model training, Epoch 26, Loss: 0.5859570688009262\n",
            "Dataset 3 model training, Epoch 27, Loss: 0.5852478285630544\n",
            "Dataset 3 model training, Epoch 28, Loss: 0.5822607817252478\n",
            "Dataset 3 model training, Epoch 29, Loss: 0.5787432505687078\n",
            "Dataset 3 model training, Epoch 30, Loss: 0.5761765158176422\n",
            "Dataset 3 model training, Epoch 31, Loss: 0.5797413176298142\n",
            "Dataset 3 model training, Epoch 32, Loss: 0.5738574316104254\n",
            "Dataset 3 model training, Epoch 33, Loss: 0.5754768403371175\n",
            "Dataset 3 model training, Epoch 34, Loss: 0.5742155545949936\n",
            "Dataset 3 model training, Epoch 35, Loss: 0.5728189106782278\n",
            "Dataset 3 model training, Epoch 36, Loss: 0.5699601234992345\n",
            "Dataset 3 model training, Epoch 37, Loss: 0.5706960030396779\n",
            "Dataset 3 model training, Epoch 38, Loss: 0.5703638072808583\n",
            "Dataset 3 model training, Epoch 39, Loss: 0.5672446709871292\n",
            "Dataset 3 model training, Epoch 40, Loss: 0.5678544557094574\n",
            "Dataset 3 model training, Epoch 41, Loss: 0.5676227863629659\n",
            "Dataset 3 model training, Epoch 42, Loss: 0.5628859120607376\n",
            "Dataset 3 model training, Epoch 43, Loss: 0.561468891898791\n",
            "Dataset 3 model training, Epoch 44, Loss: 0.5599255828062694\n",
            "Dataset 3 model training, Epoch 45, Loss: 0.560556823015213\n",
            "Dataset 3 model training, Epoch 46, Loss: 0.5700494903326034\n",
            "Dataset 3 model training, Epoch 47, Loss: 0.5572647323211034\n",
            "Dataset 3 model training, Epoch 48, Loss: 0.5591097456216813\n",
            "Dataset 3 model training, Epoch 49, Loss: 0.5560493695735932\n",
            "Dataset 3 model training, Epoch 50, Loss: 0.5615009605884552\n",
            "Dataset 3 model training, Epoch 51, Loss: 0.5566781932115554\n",
            "Dataset 3 model training, Epoch 52, Loss: 0.5686034373442332\n",
            "Dataset 3 model training, Epoch 53, Loss: 0.5484889501333237\n",
            "Dataset 3 model training, Epoch 54, Loss: 0.5522006742159525\n",
            "Dataset 3 model training, Epoch 55, Loss: 0.5531766533851623\n",
            "Dataset 3 model training, Epoch 56, Loss: 0.5553456195195516\n",
            "Dataset 3 model training, Epoch 57, Loss: 0.5523545549313227\n",
            "Dataset 3 model training, Epoch 58, Loss: 0.5480492071310679\n",
            "Dataset 3 model training, Epoch 59, Loss: 0.5484887005885443\n",
            "Dataset 3 model training, Epoch 60, Loss: 0.5549459085861842\n",
            "Dataset 3 model training, Epoch 61, Loss: 0.548861051996549\n",
            "Dataset 3 model training, Epoch 62, Loss: 0.5507935684919357\n",
            "Dataset 3 model training, Epoch 63, Loss: 0.5487427224715551\n",
            "Dataset 3 model training, Epoch 64, Loss: 0.5516412309805552\n",
            "Dataset 3 model training, Epoch 65, Loss: 0.540757396419843\n",
            "Dataset 3 model training, Epoch 66, Loss: 0.5376868416865667\n",
            "Dataset 3 model training, Epoch 67, Loss: 0.5335845706860224\n",
            "Dataset 3 model training, Epoch 68, Loss: 0.5381707559029262\n",
            "Dataset 3 model training, Epoch 69, Loss: 0.5277971124649048\n",
            "Dataset 3 model training, Epoch 70, Loss: 0.5315307986736297\n",
            "Dataset 3 model training, Epoch 71, Loss: 0.5363111793994904\n",
            "Dataset 3 model training, Epoch 72, Loss: 0.534240541656812\n",
            "Dataset 3 model training, Epoch 73, Loss: 0.5335779277483622\n",
            "Dataset 3 model training, Epoch 74, Loss: 0.5271004913250605\n",
            "Dataset 3 model training, Epoch 75, Loss: 0.5334171066681545\n",
            "Dataset 3 model training, Epoch 76, Loss: 0.5307286757230759\n",
            "Dataset 3 model training, Epoch 77, Loss: 0.5302178076903026\n",
            "Dataset 3 model training, Epoch 78, Loss: 0.5241547761360804\n",
            "Dataset 3 model training, Epoch 79, Loss: 0.5256204066673914\n",
            "Dataset 3 model training, Epoch 80, Loss: 0.5215012526512146\n",
            "Dataset 3 model training, Epoch 81, Loss: 0.5242571792999904\n",
            "Dataset 3 model training, Epoch 82, Loss: 0.5328823894262313\n",
            "Dataset 3 model training, Epoch 83, Loss: 0.5211032593250274\n",
            "Dataset 3 model training, Epoch 84, Loss: 0.5280572376648585\n",
            "Dataset 3 model training, Epoch 85, Loss: 0.5330757103363672\n",
            "Dataset 3 model training, Epoch 86, Loss: 0.5207455762227376\n",
            "Dataset 3 model training, Epoch 87, Loss: 0.5216739803552628\n",
            "Dataset 3 model training, Epoch 88, Loss: 0.5263270292679468\n",
            "Dataset 3 model training, Epoch 89, Loss: 0.5321716344356537\n",
            "Dataset 3 model training, Epoch 90, Loss: 0.5270951771736145\n",
            "Dataset 3 model training, Epoch 91, Loss: 0.5206954125563303\n",
            "Dataset 3 model training, Epoch 92, Loss: 0.5142094097534815\n",
            "Dataset 3 model training, Epoch 93, Loss: 0.5322675234079361\n",
            "Dataset 3 model training, Epoch 94, Loss: 0.5233844979604085\n",
            "Dataset 3 model training, Epoch 95, Loss: 0.5195984111229579\n",
            "Dataset 3 model training, Epoch 96, Loss: 0.5274560068051021\n",
            "Dataset 3 model training, Epoch 97, Loss: 0.5227072294553121\n",
            "Dataset 3 model training, Epoch 98, Loss: 0.5299007113774618\n",
            "Dataset 3 model training, Epoch 99, Loss: 0.5177184784412384\n",
            "Dataset 3 model training, Epoch 100, Loss: 0.5161118793487549\n",
            "Dataset 3 model training, Epoch 101, Loss: 0.5189174371957779\n",
            "Early stopping triggered for Dataset 3 model after epoch 102\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 4/15\n",
            "Dataset 4 model training, Epoch 1, Loss: 1.4242342884027506\n",
            "Dataset 4 model training, Epoch 2, Loss: 1.1464158085328113\n",
            "Dataset 4 model training, Epoch 3, Loss: 0.9945847075196761\n",
            "Dataset 4 model training, Epoch 4, Loss: 0.8983529990986933\n",
            "Dataset 4 model training, Epoch 5, Loss: 0.8616898003258283\n",
            "Dataset 4 model training, Epoch 6, Loss: 0.8215139866629734\n",
            "Dataset 4 model training, Epoch 7, Loss: 0.8005917981455598\n",
            "Dataset 4 model training, Epoch 8, Loss: 0.7833287946031063\n",
            "Dataset 4 model training, Epoch 9, Loss: 0.7687506475780583\n",
            "Dataset 4 model training, Epoch 10, Loss: 0.7554305686226374\n",
            "Dataset 4 model training, Epoch 11, Loss: 0.7399354384669775\n",
            "Dataset 4 model training, Epoch 12, Loss: 0.7260599736171433\n",
            "Dataset 4 model training, Epoch 13, Loss: 0.7172148823738098\n",
            "Dataset 4 model training, Epoch 14, Loss: 0.7044999965384037\n",
            "Dataset 4 model training, Epoch 15, Loss: 0.6916214819950394\n",
            "Dataset 4 model training, Epoch 16, Loss: 0.678419856708261\n",
            "Dataset 4 model training, Epoch 17, Loss: 0.667756812104696\n",
            "Dataset 4 model training, Epoch 18, Loss: 0.6646201508709147\n",
            "Dataset 4 model training, Epoch 19, Loss: 0.6604849092568024\n",
            "Dataset 4 model training, Epoch 20, Loss: 0.6507144815559629\n",
            "Dataset 4 model training, Epoch 21, Loss: 0.6400151143345651\n",
            "Dataset 4 model training, Epoch 22, Loss: 0.6458486624156372\n",
            "Dataset 4 model training, Epoch 23, Loss: 0.6338992809193044\n",
            "Dataset 4 model training, Epoch 24, Loss: 0.627415993734251\n",
            "Dataset 4 model training, Epoch 25, Loss: 0.6196698691648773\n",
            "Dataset 4 model training, Epoch 26, Loss: 0.6257110304093059\n",
            "Dataset 4 model training, Epoch 27, Loss: 0.6273741657975354\n",
            "Dataset 4 model training, Epoch 28, Loss: 0.6211910730675806\n",
            "Dataset 4 model training, Epoch 29, Loss: 0.6138568247043634\n",
            "Dataset 4 model training, Epoch 30, Loss: 0.6125977871161473\n",
            "Dataset 4 model training, Epoch 31, Loss: 0.6152349487890171\n",
            "Dataset 4 model training, Epoch 32, Loss: 0.6083219809622704\n",
            "Dataset 4 model training, Epoch 33, Loss: 0.6070180808818793\n",
            "Dataset 4 model training, Epoch 34, Loss: 0.6066486254523072\n",
            "Dataset 4 model training, Epoch 35, Loss: 0.6058328361073627\n",
            "Dataset 4 model training, Epoch 36, Loss: 0.6094741540241845\n",
            "Dataset 4 model training, Epoch 37, Loss: 0.5955921605795245\n",
            "Dataset 4 model training, Epoch 38, Loss: 0.5977478742222243\n",
            "Dataset 4 model training, Epoch 39, Loss: 0.6015943043594119\n",
            "Dataset 4 model training, Epoch 40, Loss: 0.5942270967024791\n",
            "Dataset 4 model training, Epoch 41, Loss: 0.5991022666043873\n",
            "Dataset 4 model training, Epoch 42, Loss: 0.5943361430228511\n",
            "Dataset 4 model training, Epoch 43, Loss: 0.5961938896511174\n",
            "Dataset 4 model training, Epoch 44, Loss: 0.5959098986055278\n",
            "Dataset 4 model training, Epoch 45, Loss: 0.5918725634677501\n",
            "Dataset 4 model training, Epoch 46, Loss: 0.5968931736070898\n",
            "Dataset 4 model training, Epoch 47, Loss: 0.5917908311644687\n",
            "Dataset 4 model training, Epoch 48, Loss: 0.5967261042398743\n",
            "Dataset 4 model training, Epoch 49, Loss: 0.588950453113906\n",
            "Dataset 4 model training, Epoch 50, Loss: 0.6020784213950362\n",
            "Dataset 4 model training, Epoch 51, Loss: 0.5860664610621296\n",
            "Dataset 4 model training, Epoch 52, Loss: 0.5945797264575958\n",
            "Dataset 4 model training, Epoch 53, Loss: 0.5876791941214211\n",
            "Dataset 4 model training, Epoch 54, Loss: 0.5917958749245994\n",
            "Dataset 4 model training, Epoch 55, Loss: 0.5930144224740281\n",
            "Dataset 4 model training, Epoch 56, Loss: 0.5952109090889557\n",
            "Dataset 4 model training, Epoch 57, Loss: 0.5961340743152401\n",
            "Dataset 4 model training, Epoch 58, Loss: 0.5795791028421137\n",
            "Dataset 4 model training, Epoch 59, Loss: 0.5798596602074707\n",
            "Dataset 4 model training, Epoch 60, Loss: 0.5777668232404733\n",
            "Dataset 4 model training, Epoch 61, Loss: 0.5729159405337104\n",
            "Dataset 4 model training, Epoch 62, Loss: 0.5685616094100324\n",
            "Dataset 4 model training, Epoch 63, Loss: 0.5743316740174836\n",
            "Dataset 4 model training, Epoch 64, Loss: 0.5700989476864851\n",
            "Dataset 4 model training, Epoch 65, Loss: 0.5720855638950686\n",
            "Dataset 4 model training, Epoch 66, Loss: 0.5615482415202298\n",
            "Dataset 4 model training, Epoch 67, Loss: 0.5717422279376018\n",
            "Dataset 4 model training, Epoch 68, Loss: 0.5726660454197775\n",
            "Dataset 4 model training, Epoch 69, Loss: 0.5775545820405211\n",
            "Dataset 4 model training, Epoch 70, Loss: 0.5733952069584327\n",
            "Dataset 4 model training, Epoch 71, Loss: 0.5734014358324341\n",
            "Dataset 4 model training, Epoch 72, Loss: 0.5695621252437181\n",
            "Dataset 4 model training, Epoch 73, Loss: 0.5596330062120776\n",
            "Dataset 4 model training, Epoch 74, Loss: 0.5587209104737149\n",
            "Dataset 4 model training, Epoch 75, Loss: 0.5525165694423869\n",
            "Dataset 4 model training, Epoch 76, Loss: 0.5555479166251195\n",
            "Dataset 4 model training, Epoch 77, Loss: 0.5582260063177422\n",
            "Dataset 4 model training, Epoch 78, Loss: 0.5601093746438811\n",
            "Dataset 4 model training, Epoch 79, Loss: 0.5522327051509784\n",
            "Dataset 4 model training, Epoch 80, Loss: 0.5596251657491997\n",
            "Dataset 4 model training, Epoch 81, Loss: 0.5545657648693157\n",
            "Dataset 4 model training, Epoch 82, Loss: 0.5563060199912591\n",
            "Dataset 4 model training, Epoch 83, Loss: 0.5587837462183796\n",
            "Dataset 4 model training, Epoch 84, Loss: 0.5572564452886581\n",
            "Dataset 4 model training, Epoch 85, Loss: 0.5554229096521305\n",
            "Dataset 4 model training, Epoch 86, Loss: 0.5519428981255882\n",
            "Dataset 4 model training, Epoch 87, Loss: 0.5487653879047949\n",
            "Dataset 4 model training, Epoch 88, Loss: 0.5497393013933037\n",
            "Dataset 4 model training, Epoch 89, Loss: 0.5436423518989659\n",
            "Dataset 4 model training, Epoch 90, Loss: 0.5492534569547146\n",
            "Dataset 4 model training, Epoch 91, Loss: 0.5401418278866177\n",
            "Dataset 4 model training, Epoch 92, Loss: 0.5383679491432407\n",
            "Dataset 4 model training, Epoch 93, Loss: 0.5365466979108279\n",
            "Dataset 4 model training, Epoch 94, Loss: 0.5512086537819875\n",
            "Dataset 4 model training, Epoch 95, Loss: 0.5475647189194643\n",
            "Dataset 4 model training, Epoch 96, Loss: 0.5432182954082007\n",
            "Dataset 4 model training, Epoch 97, Loss: 0.5463302022294153\n",
            "Dataset 4 model training, Epoch 98, Loss: 0.5390978689435162\n",
            "Dataset 4 model training, Epoch 99, Loss: 0.5392338571291936\n",
            "Dataset 4 model training, Epoch 100, Loss: 0.5420509403264975\n",
            "Dataset 4 model training, Epoch 101, Loss: 0.5350047193373306\n",
            "Dataset 4 model training, Epoch 102, Loss: 0.5426966904462138\n",
            "Dataset 4 model training, Epoch 103, Loss: 0.5375016695713695\n",
            "Dataset 4 model training, Epoch 104, Loss: 0.535725961380367\n",
            "Dataset 4 model training, Epoch 105, Loss: 0.5367828434026694\n",
            "Dataset 4 model training, Epoch 106, Loss: 0.5401610877317718\n",
            "Dataset 4 model training, Epoch 107, Loss: 0.5358334404381015\n",
            "Dataset 4 model training, Epoch 108, Loss: 0.5358691260784487\n",
            "Dataset 4 model training, Epoch 109, Loss: 0.5315408674599249\n",
            "Dataset 4 model training, Epoch 110, Loss: 0.5329475523550299\n",
            "Dataset 4 model training, Epoch 111, Loss: 0.532852247734613\n",
            "Dataset 4 model training, Epoch 112, Loss: 0.5380056798458099\n",
            "Dataset 4 model training, Epoch 113, Loss: 0.5328596581386614\n",
            "Dataset 4 model training, Epoch 114, Loss: 0.5336280440605139\n",
            "Dataset 4 model training, Epoch 115, Loss: 0.5358627429114112\n",
            "Dataset 4 model training, Epoch 116, Loss: 0.5315126142924345\n",
            "Dataset 4 model training, Epoch 117, Loss: 0.5311723006299779\n",
            "Dataset 4 model training, Epoch 118, Loss: 0.536938235352311\n",
            "Dataset 4 model training, Epoch 119, Loss: 0.5233207942941521\n",
            "Dataset 4 model training, Epoch 120, Loss: 0.5298891009031972\n",
            "Dataset 4 model training, Epoch 121, Loss: 0.5291146081459673\n",
            "Dataset 4 model training, Epoch 122, Loss: 0.5259500885311561\n",
            "Dataset 4 model training, Epoch 123, Loss: 0.5300505355566363\n",
            "Dataset 4 model training, Epoch 124, Loss: 0.5340274647821354\n",
            "Dataset 4 model training, Epoch 125, Loss: 0.5313974850917165\n",
            "Dataset 4 model training, Epoch 126, Loss: 0.5358573366192323\n",
            "Dataset 4 model training, Epoch 127, Loss: 0.5355774207582956\n",
            "Dataset 4 model training, Epoch 128, Loss: 0.5310227955821194\n",
            "Early stopping triggered for Dataset 4 model after epoch 129\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 5/15\n",
            "Dataset 5 model training, Epoch 1, Loss: 1.4574607034657625\n",
            "Dataset 5 model training, Epoch 2, Loss: 1.2151516299919793\n",
            "Dataset 5 model training, Epoch 3, Loss: 1.0617040487743865\n",
            "Dataset 5 model training, Epoch 4, Loss: 0.9622464492016991\n",
            "Dataset 5 model training, Epoch 5, Loss: 0.9045282922335119\n",
            "Dataset 5 model training, Epoch 6, Loss: 0.8713336546949092\n",
            "Dataset 5 model training, Epoch 7, Loss: 0.8386881779504303\n",
            "Dataset 5 model training, Epoch 8, Loss: 0.8212442666092175\n",
            "Dataset 5 model training, Epoch 9, Loss: 0.799676227889605\n",
            "Dataset 5 model training, Epoch 10, Loss: 0.7955255036386067\n",
            "Dataset 5 model training, Epoch 11, Loss: 0.7767948560266686\n",
            "Dataset 5 model training, Epoch 12, Loss: 0.7649309611160483\n",
            "Dataset 5 model training, Epoch 13, Loss: 0.750592159344846\n",
            "Dataset 5 model training, Epoch 14, Loss: 0.7459869908806461\n",
            "Dataset 5 model training, Epoch 15, Loss: 0.7286759518936976\n",
            "Dataset 5 model training, Epoch 16, Loss: 0.7126463323631542\n",
            "Dataset 5 model training, Epoch 17, Loss: 0.7130108755706941\n",
            "Dataset 5 model training, Epoch 18, Loss: 0.699129863473393\n",
            "Dataset 5 model training, Epoch 19, Loss: 0.692296462971092\n",
            "Dataset 5 model training, Epoch 20, Loss: 0.68824445361259\n",
            "Dataset 5 model training, Epoch 21, Loss: 0.6882571894050444\n",
            "Dataset 5 model training, Epoch 22, Loss: 0.6781760106150736\n",
            "Dataset 5 model training, Epoch 23, Loss: 0.6746962418492208\n",
            "Dataset 5 model training, Epoch 24, Loss: 0.6705906059117925\n",
            "Dataset 5 model training, Epoch 25, Loss: 0.665622062331078\n",
            "Dataset 5 model training, Epoch 26, Loss: 0.6664469554120263\n",
            "Dataset 5 model training, Epoch 27, Loss: 0.663540644133651\n",
            "Dataset 5 model training, Epoch 28, Loss: 0.6571725054875316\n",
            "Dataset 5 model training, Epoch 29, Loss: 0.6628022905964179\n",
            "Dataset 5 model training, Epoch 30, Loss: 0.6630309782172209\n",
            "Dataset 5 model training, Epoch 31, Loss: 0.6592840376316301\n",
            "Dataset 5 model training, Epoch 32, Loss: 0.6580779048420439\n",
            "Dataset 5 model training, Epoch 33, Loss: 0.647491894312353\n",
            "Dataset 5 model training, Epoch 34, Loss: 0.6572673216762158\n",
            "Dataset 5 model training, Epoch 35, Loss: 0.6497230565787961\n",
            "Dataset 5 model training, Epoch 36, Loss: 0.6530262815872295\n",
            "Dataset 5 model training, Epoch 37, Loss: 0.6545862305484362\n",
            "Dataset 5 model training, Epoch 38, Loss: 0.6505922355107813\n",
            "Dataset 5 model training, Epoch 39, Loss: 0.6490967241709664\n",
            "Dataset 5 model training, Epoch 40, Loss: 0.6443502710969656\n",
            "Dataset 5 model training, Epoch 41, Loss: 0.6462982616968603\n",
            "Dataset 5 model training, Epoch 42, Loss: 0.6453961129956598\n",
            "Dataset 5 model training, Epoch 43, Loss: 0.6378423979618405\n",
            "Dataset 5 model training, Epoch 44, Loss: 0.6366306667359883\n",
            "Dataset 5 model training, Epoch 45, Loss: 0.6381732193415597\n",
            "Dataset 5 model training, Epoch 46, Loss: 0.6407614530332937\n",
            "Dataset 5 model training, Epoch 47, Loss: 0.6366786996790227\n",
            "Dataset 5 model training, Epoch 48, Loss: 0.6309593265888674\n",
            "Dataset 5 model training, Epoch 49, Loss: 0.6356435050100289\n",
            "Dataset 5 model training, Epoch 50, Loss: 0.6326843666150266\n",
            "Dataset 5 model training, Epoch 51, Loss: 0.6350897390570417\n",
            "Dataset 5 model training, Epoch 52, Loss: 0.6331067029261749\n",
            "Dataset 5 model training, Epoch 53, Loss: 0.6338218888580399\n",
            "Dataset 5 model training, Epoch 54, Loss: 0.6297541696753278\n",
            "Dataset 5 model training, Epoch 55, Loss: 0.6297873648621092\n",
            "Dataset 5 model training, Epoch 56, Loss: 0.6357731873157041\n",
            "Dataset 5 model training, Epoch 57, Loss: 0.6342847059237077\n",
            "Dataset 5 model training, Epoch 58, Loss: 0.6235758808235194\n",
            "Dataset 5 model training, Epoch 59, Loss: 0.6367629920476235\n",
            "Dataset 5 model training, Epoch 60, Loss: 0.6311584459055191\n",
            "Dataset 5 model training, Epoch 61, Loss: 0.6324464228729274\n",
            "Dataset 5 model training, Epoch 62, Loss: 0.6291752355610765\n",
            "Dataset 5 model training, Epoch 63, Loss: 0.6278853656461575\n",
            "Dataset 5 model training, Epoch 64, Loss: 0.6279003016100634\n",
            "Dataset 5 model training, Epoch 65, Loss: 0.6196561105699347\n",
            "Dataset 5 model training, Epoch 66, Loss: 0.6226240236487165\n",
            "Dataset 5 model training, Epoch 67, Loss: 0.6169848626091976\n",
            "Dataset 5 model training, Epoch 68, Loss: 0.6195085888741\n",
            "Dataset 5 model training, Epoch 69, Loss: 0.6230194102597717\n",
            "Dataset 5 model training, Epoch 70, Loss: 0.6130343007561344\n",
            "Dataset 5 model training, Epoch 71, Loss: 0.6121608068879019\n",
            "Dataset 5 model training, Epoch 72, Loss: 0.613951555634505\n",
            "Dataset 5 model training, Epoch 73, Loss: 0.6161830983305937\n",
            "Dataset 5 model training, Epoch 74, Loss: 0.6132242863610287\n",
            "Dataset 5 model training, Epoch 75, Loss: 0.6121646423867885\n",
            "Dataset 5 model training, Epoch 76, Loss: 0.6095859698401201\n",
            "Dataset 5 model training, Epoch 77, Loss: 0.6134429229985947\n",
            "Dataset 5 model training, Epoch 78, Loss: 0.6133053846807288\n",
            "Dataset 5 model training, Epoch 79, Loss: 0.6118859628702971\n",
            "Dataset 5 model training, Epoch 80, Loss: 0.6144383655698508\n",
            "Dataset 5 model training, Epoch 81, Loss: 0.6123944440944082\n",
            "Dataset 5 model training, Epoch 82, Loss: 0.6184598789519111\n",
            "Dataset 5 model training, Epoch 83, Loss: 0.602363502419235\n",
            "Dataset 5 model training, Epoch 84, Loss: 0.6023014659849589\n",
            "Dataset 5 model training, Epoch 85, Loss: 0.604485573784617\n",
            "Dataset 5 model training, Epoch 86, Loss: 0.598544343405922\n",
            "Dataset 5 model training, Epoch 87, Loss: 0.6058482581737058\n",
            "Dataset 5 model training, Epoch 88, Loss: 0.5987109777911398\n",
            "Dataset 5 model training, Epoch 89, Loss: 0.5985394192221981\n",
            "Dataset 5 model training, Epoch 90, Loss: 0.5940955177249524\n",
            "Dataset 5 model training, Epoch 91, Loss: 0.5965278960714404\n",
            "Dataset 5 model training, Epoch 92, Loss: 0.5927553576911055\n",
            "Dataset 5 model training, Epoch 93, Loss: 0.5961774719641513\n",
            "Dataset 5 model training, Epoch 94, Loss: 0.5975653879194451\n",
            "Dataset 5 model training, Epoch 95, Loss: 0.5977260952427883\n",
            "Dataset 5 model training, Epoch 96, Loss: 0.5937440969399957\n",
            "Dataset 5 model training, Epoch 97, Loss: 0.6007010160676585\n",
            "Dataset 5 model training, Epoch 98, Loss: 0.5915901914938985\n",
            "Dataset 5 model training, Epoch 99, Loss: 0.5969272981954101\n",
            "Dataset 5 model training, Epoch 100, Loss: 0.5885346401857849\n",
            "Dataset 5 model training, Epoch 101, Loss: 0.5907344580096686\n",
            "Dataset 5 model training, Epoch 102, Loss: 0.5952245565068802\n",
            "Dataset 5 model training, Epoch 103, Loss: 0.5942764222221887\n",
            "Dataset 5 model training, Epoch 104, Loss: 0.593854969779917\n",
            "Dataset 5 model training, Epoch 105, Loss: 0.5961396434163088\n",
            "Dataset 5 model training, Epoch 106, Loss: 0.5941789902296643\n",
            "Dataset 5 model training, Epoch 107, Loss: 0.5942006723192714\n",
            "Dataset 5 model training, Epoch 108, Loss: 0.5895446612930938\n",
            "Dataset 5 model training, Epoch 109, Loss: 0.5977628761089888\n",
            "Early stopping triggered for Dataset 5 model after epoch 110\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 6/15\n",
            "Dataset 6 model training, Epoch 1, Loss: 1.4040624761581422\n",
            "Dataset 6 model training, Epoch 2, Loss: 1.1439646355311075\n",
            "Dataset 6 model training, Epoch 3, Loss: 0.9919852268695831\n",
            "Dataset 6 model training, Epoch 4, Loss: 0.9020837148030599\n",
            "Dataset 6 model training, Epoch 5, Loss: 0.8448921545346578\n",
            "Dataset 6 model training, Epoch 6, Loss: 0.7983669364452362\n",
            "Dataset 6 model training, Epoch 7, Loss: 0.7721341057618459\n",
            "Dataset 6 model training, Epoch 8, Loss: 0.7543384897708892\n",
            "Dataset 6 model training, Epoch 9, Loss: 0.7437702000141144\n",
            "Dataset 6 model training, Epoch 10, Loss: 0.7274813489119212\n",
            "Dataset 6 model training, Epoch 11, Loss: 0.7000049901008606\n",
            "Dataset 6 model training, Epoch 12, Loss: 0.6976557977994283\n",
            "Dataset 6 model training, Epoch 13, Loss: 0.6782209825515747\n",
            "Dataset 6 model training, Epoch 14, Loss: 0.6693153166770935\n",
            "Dataset 6 model training, Epoch 15, Loss: 0.6494937539100647\n",
            "Dataset 6 model training, Epoch 16, Loss: 0.6330974012613296\n",
            "Dataset 6 model training, Epoch 17, Loss: 0.6304763654867808\n",
            "Dataset 6 model training, Epoch 18, Loss: 0.6159026747941971\n",
            "Dataset 6 model training, Epoch 19, Loss: 0.6076353865861893\n",
            "Dataset 6 model training, Epoch 20, Loss: 0.6013245993852615\n",
            "Dataset 6 model training, Epoch 21, Loss: 0.5868126241366068\n",
            "Dataset 6 model training, Epoch 22, Loss: 0.577524129152298\n",
            "Dataset 6 model training, Epoch 23, Loss: 0.5729655192295711\n",
            "Dataset 6 model training, Epoch 24, Loss: 0.5677721051375071\n",
            "Dataset 6 model training, Epoch 25, Loss: 0.5683108166853587\n",
            "Dataset 6 model training, Epoch 26, Loss: 0.5652833400170009\n",
            "Dataset 6 model training, Epoch 27, Loss: 0.5670622112353643\n",
            "Dataset 6 model training, Epoch 28, Loss: 0.5558896541595459\n",
            "Dataset 6 model training, Epoch 29, Loss: 0.556070572535197\n",
            "Dataset 6 model training, Epoch 30, Loss: 0.557260468006134\n",
            "Dataset 6 model training, Epoch 31, Loss: 0.5532502951224645\n",
            "Dataset 6 model training, Epoch 32, Loss: 0.5556271201372147\n",
            "Dataset 6 model training, Epoch 33, Loss: 0.5577238810062408\n",
            "Dataset 6 model training, Epoch 34, Loss: 0.5546159029006958\n",
            "Dataset 6 model training, Epoch 35, Loss: 0.5430010851224264\n",
            "Dataset 6 model training, Epoch 36, Loss: 0.5524592729409535\n",
            "Dataset 6 model training, Epoch 37, Loss: 0.5492501759529114\n",
            "Dataset 6 model training, Epoch 38, Loss: 0.5487682084242503\n",
            "Dataset 6 model training, Epoch 39, Loss: 0.5444620776176453\n",
            "Dataset 6 model training, Epoch 40, Loss: 0.5505304970343907\n",
            "Dataset 6 model training, Epoch 41, Loss: 0.5440189749002456\n",
            "Dataset 6 model training, Epoch 42, Loss: 0.5390881635745366\n",
            "Dataset 6 model training, Epoch 43, Loss: 0.5349156510829925\n",
            "Dataset 6 model training, Epoch 44, Loss: 0.5369285847743352\n",
            "Dataset 6 model training, Epoch 45, Loss: 0.5397913992404938\n",
            "Dataset 6 model training, Epoch 46, Loss: 0.5387800168991089\n",
            "Dataset 6 model training, Epoch 47, Loss: 0.5358992687861125\n",
            "Dataset 6 model training, Epoch 48, Loss: 0.5345760059356689\n",
            "Dataset 6 model training, Epoch 49, Loss: 0.5342634046077728\n",
            "Dataset 6 model training, Epoch 50, Loss: 0.532671932776769\n",
            "Dataset 6 model training, Epoch 51, Loss: 0.5325428326924642\n",
            "Dataset 6 model training, Epoch 52, Loss: 0.5330041088660558\n",
            "Dataset 6 model training, Epoch 53, Loss: 0.5352995423475901\n",
            "Dataset 6 model training, Epoch 54, Loss: 0.5312248148520787\n",
            "Dataset 6 model training, Epoch 55, Loss: 0.5339464531342188\n",
            "Dataset 6 model training, Epoch 56, Loss: 0.5355551487207413\n",
            "Dataset 6 model training, Epoch 57, Loss: 0.5322562358776728\n",
            "Dataset 6 model training, Epoch 58, Loss: 0.5311997906366984\n",
            "Dataset 6 model training, Epoch 59, Loss: 0.5304691869020463\n",
            "Dataset 6 model training, Epoch 60, Loss: 0.53730537712574\n",
            "Dataset 6 model training, Epoch 61, Loss: 0.5298179856936137\n",
            "Dataset 6 model training, Epoch 62, Loss: 0.5346585446596146\n",
            "Dataset 6 model training, Epoch 63, Loss: 0.5332683155934016\n",
            "Dataset 6 model training, Epoch 64, Loss: 0.5377258924643199\n",
            "Dataset 6 model training, Epoch 65, Loss: 0.5293857683738072\n",
            "Dataset 6 model training, Epoch 66, Loss: 0.5308044854799906\n",
            "Dataset 6 model training, Epoch 67, Loss: 0.5284906889994939\n",
            "Dataset 6 model training, Epoch 68, Loss: 0.532581045627594\n",
            "Dataset 6 model training, Epoch 69, Loss: 0.5341268302996953\n",
            "Dataset 6 model training, Epoch 70, Loss: 0.5347168894608816\n",
            "Dataset 6 model training, Epoch 71, Loss: 0.5329747911294301\n",
            "Dataset 6 model training, Epoch 72, Loss: 0.5336954587697983\n",
            "Dataset 6 model training, Epoch 73, Loss: 0.5322266012430191\n",
            "Dataset 6 model training, Epoch 74, Loss: 0.528352237145106\n",
            "Dataset 6 model training, Epoch 75, Loss: 0.52278706630071\n",
            "Dataset 6 model training, Epoch 76, Loss: 0.5265759660800298\n",
            "Dataset 6 model training, Epoch 77, Loss: 0.5253586610158284\n",
            "Dataset 6 model training, Epoch 78, Loss: 0.5182186828056972\n",
            "Dataset 6 model training, Epoch 79, Loss: 0.5213357490301133\n",
            "Dataset 6 model training, Epoch 80, Loss: 0.5234120704730352\n",
            "Dataset 6 model training, Epoch 81, Loss: 0.51871912697951\n",
            "Dataset 6 model training, Epoch 82, Loss: 0.5254886368910472\n",
            "Dataset 6 model training, Epoch 83, Loss: 0.5219325222571691\n",
            "Dataset 6 model training, Epoch 84, Loss: 0.5239881946643193\n",
            "Dataset 6 model training, Epoch 85, Loss: 0.5245331621170044\n",
            "Dataset 6 model training, Epoch 86, Loss: 0.5237382858991623\n",
            "Dataset 6 model training, Epoch 87, Loss: 0.5160665035247802\n",
            "Dataset 6 model training, Epoch 88, Loss: 0.5190151260296504\n",
            "Dataset 6 model training, Epoch 89, Loss: 0.515887142419815\n",
            "Dataset 6 model training, Epoch 90, Loss: 0.5154473235209783\n",
            "Dataset 6 model training, Epoch 91, Loss: 0.5189084758361181\n",
            "Dataset 6 model training, Epoch 92, Loss: 0.5145933332045873\n",
            "Dataset 6 model training, Epoch 93, Loss: 0.518288777867953\n",
            "Dataset 6 model training, Epoch 94, Loss: 0.513618713816007\n",
            "Dataset 6 model training, Epoch 95, Loss: 0.5177040815353393\n",
            "Dataset 6 model training, Epoch 96, Loss: 0.5198226646582286\n",
            "Dataset 6 model training, Epoch 97, Loss: 0.5186853784322739\n",
            "Dataset 6 model training, Epoch 98, Loss: 0.513858058253924\n",
            "Dataset 6 model training, Epoch 99, Loss: 0.5170555112759272\n",
            "Dataset 6 model training, Epoch 100, Loss: 0.5182933044433594\n",
            "Dataset 6 model training, Epoch 101, Loss: 0.51578236023585\n",
            "Dataset 6 model training, Epoch 102, Loss: 0.5155165884892146\n",
            "Dataset 6 model training, Epoch 103, Loss: 0.516097959280014\n",
            "Early stopping triggered for Dataset 6 model after epoch 104\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 7/15\n",
            "Dataset 7 model training, Epoch 1, Loss: 1.417610316663175\n",
            "Dataset 7 model training, Epoch 2, Loss: 1.153768922026093\n",
            "Dataset 7 model training, Epoch 3, Loss: 1.0048671030514948\n",
            "Dataset 7 model training, Epoch 4, Loss: 0.9017961295069875\n",
            "Dataset 7 model training, Epoch 5, Loss: 0.8407378824981483\n",
            "Dataset 7 model training, Epoch 6, Loss: 0.8069783763305561\n",
            "Dataset 7 model training, Epoch 7, Loss: 0.7708276357199695\n",
            "Dataset 7 model training, Epoch 8, Loss: 0.7635044004466083\n",
            "Dataset 7 model training, Epoch 9, Loss: 0.7418934811611433\n",
            "Dataset 7 model training, Epoch 10, Loss: 0.7295523467096122\n",
            "Dataset 7 model training, Epoch 11, Loss: 0.7116225035609426\n",
            "Dataset 7 model training, Epoch 12, Loss: 0.7131938801423924\n",
            "Dataset 7 model training, Epoch 13, Loss: 0.6916457946236069\n",
            "Dataset 7 model training, Epoch 14, Loss: 0.6826398634427303\n",
            "Dataset 7 model training, Epoch 15, Loss: 0.6713853808673652\n",
            "Dataset 7 model training, Epoch 16, Loss: 0.6699086133692715\n",
            "Dataset 7 model training, Epoch 17, Loss: 0.6562034679022996\n",
            "Dataset 7 model training, Epoch 18, Loss: 0.6425096046280216\n",
            "Dataset 7 model training, Epoch 19, Loss: 0.6398409045225865\n",
            "Dataset 7 model training, Epoch 20, Loss: 0.6330152387554581\n",
            "Dataset 7 model training, Epoch 21, Loss: 0.6184843812842626\n",
            "Dataset 7 model training, Epoch 22, Loss: 0.6279848737491144\n",
            "Dataset 7 model training, Epoch 23, Loss: 0.6203069221731778\n",
            "Dataset 7 model training, Epoch 24, Loss: 0.614676982567117\n",
            "Dataset 7 model training, Epoch 25, Loss: 0.6066972283092705\n",
            "Dataset 7 model training, Epoch 26, Loss: 0.6025769698458749\n",
            "Dataset 7 model training, Epoch 27, Loss: 0.6023145060281496\n",
            "Dataset 7 model training, Epoch 28, Loss: 0.6009555068370458\n",
            "Dataset 7 model training, Epoch 29, Loss: 0.5971832031736503\n",
            "Dataset 7 model training, Epoch 30, Loss: 0.5958467119851628\n",
            "Dataset 7 model training, Epoch 31, Loss: 0.6000465903733228\n",
            "Dataset 7 model training, Epoch 32, Loss: 0.593492338584887\n",
            "Dataset 7 model training, Epoch 33, Loss: 0.5952207184321171\n",
            "Dataset 7 model training, Epoch 34, Loss: 0.5900152555188617\n",
            "Dataset 7 model training, Epoch 35, Loss: 0.5923055476836256\n",
            "Dataset 7 model training, Epoch 36, Loss: 0.5872145716000248\n",
            "Dataset 7 model training, Epoch 37, Loss: 0.5835619770191811\n",
            "Dataset 7 model training, Epoch 38, Loss: 0.5899734366181735\n",
            "Dataset 7 model training, Epoch 39, Loss: 0.5972916332853807\n",
            "Dataset 7 model training, Epoch 40, Loss: 0.5827592077690202\n",
            "Dataset 7 model training, Epoch 41, Loss: 0.5837493454282349\n",
            "Dataset 7 model training, Epoch 42, Loss: 0.5863850213385917\n",
            "Dataset 7 model training, Epoch 43, Loss: 0.585188715844541\n",
            "Dataset 7 model training, Epoch 44, Loss: 0.5833574356259527\n",
            "Dataset 7 model training, Epoch 45, Loss: 0.5824670701010807\n",
            "Dataset 7 model training, Epoch 46, Loss: 0.5774118429100191\n",
            "Dataset 7 model training, Epoch 47, Loss: 0.5810568453492345\n",
            "Dataset 7 model training, Epoch 48, Loss: 0.5853964327960401\n",
            "Dataset 7 model training, Epoch 49, Loss: 0.5797409562242998\n",
            "Dataset 7 model training, Epoch 50, Loss: 0.5780769405332772\n",
            "Dataset 7 model training, Epoch 51, Loss: 0.5770464583828643\n",
            "Dataset 7 model training, Epoch 52, Loss: 0.5798195558222564\n",
            "Dataset 7 model training, Epoch 53, Loss: 0.5680565767594286\n",
            "Dataset 7 model training, Epoch 54, Loss: 0.5763557343869596\n",
            "Dataset 7 model training, Epoch 55, Loss: 0.580693968244501\n",
            "Dataset 7 model training, Epoch 56, Loss: 0.5758897402399296\n",
            "Dataset 7 model training, Epoch 57, Loss: 0.5762050278686188\n",
            "Dataset 7 model training, Epoch 58, Loss: 0.5731491394139625\n",
            "Dataset 7 model training, Epoch 59, Loss: 0.5707684946624009\n",
            "Dataset 7 model training, Epoch 60, Loss: 0.5635623641916223\n",
            "Dataset 7 model training, Epoch 61, Loss: 0.562285037660921\n",
            "Dataset 7 model training, Epoch 62, Loss: 0.5611784508502161\n",
            "Dataset 7 model training, Epoch 63, Loss: 0.5582772585991267\n",
            "Dataset 7 model training, Epoch 64, Loss: 0.56299059536006\n",
            "Dataset 7 model training, Epoch 65, Loss: 0.5583979064548338\n",
            "Dataset 7 model training, Epoch 66, Loss: 0.558307866591054\n",
            "Dataset 7 model training, Epoch 67, Loss: 0.5586599846546715\n",
            "Dataset 7 model training, Epoch 68, Loss: 0.5560022159605413\n",
            "Dataset 7 model training, Epoch 69, Loss: 0.5607405530439841\n",
            "Dataset 7 model training, Epoch 70, Loss: 0.5527021196242925\n",
            "Dataset 7 model training, Epoch 71, Loss: 0.559778430171915\n",
            "Dataset 7 model training, Epoch 72, Loss: 0.5598194866969779\n",
            "Dataset 7 model training, Epoch 73, Loss: 0.5547792327565115\n",
            "Dataset 7 model training, Epoch 74, Loss: 0.5490399031220256\n",
            "Dataset 7 model training, Epoch 75, Loss: 0.5508813652637843\n",
            "Dataset 7 model training, Epoch 76, Loss: 0.5526481006999273\n",
            "Dataset 7 model training, Epoch 77, Loss: 0.55564183600851\n",
            "Dataset 7 model training, Epoch 78, Loss: 0.5557332044920406\n",
            "Dataset 7 model training, Epoch 79, Loss: 0.5531916598210463\n",
            "Dataset 7 model training, Epoch 80, Loss: 0.5528715289122349\n",
            "Dataset 7 model training, Epoch 81, Loss: 0.5453119839768152\n",
            "Dataset 7 model training, Epoch 82, Loss: 0.5495522624334773\n",
            "Dataset 7 model training, Epoch 83, Loss: 0.538170692284365\n",
            "Dataset 7 model training, Epoch 84, Loss: 0.5371041150914656\n",
            "Dataset 7 model training, Epoch 85, Loss: 0.5368073357118143\n",
            "Dataset 7 model training, Epoch 86, Loss: 0.5378010492469814\n",
            "Dataset 7 model training, Epoch 87, Loss: 0.5386817253924705\n",
            "Dataset 7 model training, Epoch 88, Loss: 0.5324166648693986\n",
            "Dataset 7 model training, Epoch 89, Loss: 0.5450001793938715\n",
            "Dataset 7 model training, Epoch 90, Loss: 0.5398773313374132\n",
            "Dataset 7 model training, Epoch 91, Loss: 0.5413842696595836\n",
            "Dataset 7 model training, Epoch 92, Loss: 0.5407910242274001\n",
            "Dataset 7 model training, Epoch 93, Loss: 0.534190595552728\n",
            "Dataset 7 model training, Epoch 94, Loss: 0.5379547059938714\n",
            "Dataset 7 model training, Epoch 95, Loss: 0.5275990898544723\n",
            "Dataset 7 model training, Epoch 96, Loss: 0.5293458160516378\n",
            "Dataset 7 model training, Epoch 97, Loss: 0.5329743323696626\n",
            "Dataset 7 model training, Epoch 98, Loss: 0.523691942160194\n",
            "Dataset 7 model training, Epoch 99, Loss: 0.5290654650008356\n",
            "Dataset 7 model training, Epoch 100, Loss: 0.5233470229683695\n",
            "Dataset 7 model training, Epoch 101, Loss: 0.5263802233982731\n",
            "Dataset 7 model training, Epoch 102, Loss: 0.5315633200713106\n",
            "Dataset 7 model training, Epoch 103, Loss: 0.5199187621474266\n",
            "Dataset 7 model training, Epoch 104, Loss: 0.5276135627079654\n",
            "Dataset 7 model training, Epoch 105, Loss: 0.5284134568394842\n",
            "Dataset 7 model training, Epoch 106, Loss: 0.5282520794787923\n",
            "Dataset 7 model training, Epoch 107, Loss: 0.528779610991478\n",
            "Dataset 7 model training, Epoch 108, Loss: 0.52353456253941\n",
            "Dataset 7 model training, Epoch 109, Loss: 0.5240605455395337\n",
            "Dataset 7 model training, Epoch 110, Loss: 0.5247816973038621\n",
            "Dataset 7 model training, Epoch 111, Loss: 0.5260374227891097\n",
            "Dataset 7 model training, Epoch 112, Loss: 0.5181856783660682\n",
            "Dataset 7 model training, Epoch 113, Loss: 0.5212789808173437\n",
            "Dataset 7 model training, Epoch 114, Loss: 0.5135934038742168\n",
            "Dataset 7 model training, Epoch 115, Loss: 0.5209060631491043\n",
            "Dataset 7 model training, Epoch 116, Loss: 0.5198831099110681\n",
            "Dataset 7 model training, Epoch 117, Loss: 0.5157248240870398\n",
            "Dataset 7 model training, Epoch 118, Loss: 0.522896068321692\n",
            "Dataset 7 model training, Epoch 119, Loss: 0.5184036261729292\n",
            "Dataset 7 model training, Epoch 120, Loss: 0.5140580698042303\n",
            "Dataset 7 model training, Epoch 121, Loss: 0.524115894291852\n",
            "Dataset 7 model training, Epoch 122, Loss: 0.5157855871964145\n",
            "Dataset 7 model training, Epoch 123, Loss: 0.5123957762041608\n",
            "Dataset 7 model training, Epoch 124, Loss: 0.518086967436043\n",
            "Dataset 7 model training, Epoch 125, Loss: 0.5200572174948614\n",
            "Dataset 7 model training, Epoch 126, Loss: 0.5151568443791287\n",
            "Dataset 7 model training, Epoch 127, Loss: 0.5198760324636021\n",
            "Dataset 7 model training, Epoch 128, Loss: 0.5163512954840789\n",
            "Dataset 7 model training, Epoch 129, Loss: 0.5185261280150026\n",
            "Dataset 7 model training, Epoch 130, Loss: 0.5156807524932397\n",
            "Dataset 7 model training, Epoch 131, Loss: 0.5149513539430257\n",
            "Dataset 7 model training, Epoch 132, Loss: 0.5134800362425882\n",
            "Dataset 7 model training, Epoch 133, Loss: 0.5112253888636022\n",
            "Dataset 7 model training, Epoch 134, Loss: 0.5151402752947163\n",
            "Dataset 7 model training, Epoch 135, Loss: 0.5097032439064335\n",
            "Dataset 7 model training, Epoch 136, Loss: 0.5093014455727629\n",
            "Dataset 7 model training, Epoch 137, Loss: 0.5152399137213424\n",
            "Dataset 7 model training, Epoch 138, Loss: 0.5176380645181682\n",
            "Dataset 7 model training, Epoch 139, Loss: 0.5084306635969394\n",
            "Dataset 7 model training, Epoch 140, Loss: 0.5087286505747486\n",
            "Dataset 7 model training, Epoch 141, Loss: 0.5097205922813028\n",
            "Dataset 7 model training, Epoch 142, Loss: 0.5110721284070531\n",
            "Dataset 7 model training, Epoch 143, Loss: 0.5079501839102926\n",
            "Dataset 7 model training, Epoch 144, Loss: 0.505795605883405\n",
            "Dataset 7 model training, Epoch 145, Loss: 0.5054488606952332\n",
            "Dataset 7 model training, Epoch 146, Loss: 0.5109956383302405\n",
            "Dataset 7 model training, Epoch 147, Loss: 0.5143142769465575\n",
            "Dataset 7 model training, Epoch 148, Loss: 0.5145581739174353\n",
            "Dataset 7 model training, Epoch 149, Loss: 0.5080488122798301\n",
            "Dataset 7 model training, Epoch 150, Loss: 0.5063196247493899\n",
            "Dataset 7 model training, Epoch 151, Loss: 0.5198642267165957\n",
            "Dataset 7 model training, Epoch 152, Loss: 0.5164851471781731\n",
            "Dataset 7 model training, Epoch 153, Loss: 0.5151934917714145\n",
            "Dataset 7 model training, Epoch 154, Loss: 0.51082942493864\n",
            "Early stopping triggered for Dataset 7 model after epoch 155\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 8/15\n",
            "Dataset 8 model training, Epoch 1, Loss: 1.4405560853497295\n",
            "Dataset 8 model training, Epoch 2, Loss: 1.190294866193861\n",
            "Dataset 8 model training, Epoch 3, Loss: 1.0377738319787404\n",
            "Dataset 8 model training, Epoch 4, Loss: 0.950692332430974\n",
            "Dataset 8 model training, Epoch 5, Loss: 0.894627624310103\n",
            "Dataset 8 model training, Epoch 6, Loss: 0.8598934363998822\n",
            "Dataset 8 model training, Epoch 7, Loss: 0.8365683995637317\n",
            "Dataset 8 model training, Epoch 8, Loss: 0.8260972351835878\n",
            "Dataset 8 model training, Epoch 9, Loss: 0.8016994567525467\n",
            "Dataset 8 model training, Epoch 10, Loss: 0.7902591404498823\n",
            "Dataset 8 model training, Epoch 11, Loss: 0.773876451805934\n",
            "Dataset 8 model training, Epoch 12, Loss: 0.7647384045108053\n",
            "Dataset 8 model training, Epoch 13, Loss: 0.7491656877850527\n",
            "Dataset 8 model training, Epoch 14, Loss: 0.7369835576755088\n",
            "Dataset 8 model training, Epoch 15, Loss: 0.725593762909806\n",
            "Dataset 8 model training, Epoch 16, Loss: 0.7167504617831851\n",
            "Dataset 8 model training, Epoch 17, Loss: 0.6997732816126523\n",
            "Dataset 8 model training, Epoch 18, Loss: 0.6940638378962575\n",
            "Dataset 8 model training, Epoch 19, Loss: 0.6878221003001168\n",
            "Dataset 8 model training, Epoch 20, Loss: 0.6799638571355167\n",
            "Dataset 8 model training, Epoch 21, Loss: 0.6797902500069382\n",
            "Dataset 8 model training, Epoch 22, Loss: 0.6739902116308276\n",
            "Dataset 8 model training, Epoch 23, Loss: 0.6687031940325795\n",
            "Dataset 8 model training, Epoch 24, Loss: 0.6649420141373705\n",
            "Dataset 8 model training, Epoch 25, Loss: 0.6664055289838138\n",
            "Dataset 8 model training, Epoch 26, Loss: 0.6608033752281394\n",
            "Dataset 8 model training, Epoch 27, Loss: 0.664237290020757\n",
            "Dataset 8 model training, Epoch 28, Loss: 0.662570539136861\n",
            "Dataset 8 model training, Epoch 29, Loss: 0.6540002070817371\n",
            "Dataset 8 model training, Epoch 30, Loss: 0.6592981603321613\n",
            "Dataset 8 model training, Epoch 31, Loss: 0.6517682341521218\n",
            "Dataset 8 model training, Epoch 32, Loss: 0.6481224726510528\n",
            "Dataset 8 model training, Epoch 33, Loss: 0.6535716046822951\n",
            "Dataset 8 model training, Epoch 34, Loss: 0.6474592823710218\n",
            "Dataset 8 model training, Epoch 35, Loss: 0.6485274142066905\n",
            "Dataset 8 model training, Epoch 36, Loss: 0.6501640377428708\n",
            "Dataset 8 model training, Epoch 37, Loss: 0.6477560816995249\n",
            "Dataset 8 model training, Epoch 38, Loss: 0.6498867849375578\n",
            "Dataset 8 model training, Epoch 39, Loss: 0.6400190653816965\n",
            "Dataset 8 model training, Epoch 40, Loss: 0.6516152048270974\n",
            "Dataset 8 model training, Epoch 41, Loss: 0.6418170020884315\n",
            "Dataset 8 model training, Epoch 42, Loss: 0.6425692197060425\n",
            "Dataset 8 model training, Epoch 43, Loss: 0.6426349976318795\n",
            "Dataset 8 model training, Epoch 44, Loss: 0.6358592692237572\n",
            "Dataset 8 model training, Epoch 45, Loss: 0.6414498686790466\n",
            "Dataset 8 model training, Epoch 46, Loss: 0.641212032545333\n",
            "Dataset 8 model training, Epoch 47, Loss: 0.63253906609228\n",
            "Dataset 8 model training, Epoch 48, Loss: 0.6371726119678293\n",
            "Dataset 8 model training, Epoch 49, Loss: 0.6354577915380465\n",
            "Dataset 8 model training, Epoch 50, Loss: 0.6359056790403071\n",
            "Dataset 8 model training, Epoch 51, Loss: 0.6418040954826663\n",
            "Dataset 8 model training, Epoch 52, Loss: 0.6395797105443558\n",
            "Dataset 8 model training, Epoch 53, Loss: 0.6422062632221504\n",
            "Dataset 8 model training, Epoch 54, Loss: 0.6335666257663061\n",
            "Dataset 8 model training, Epoch 55, Loss: 0.6175946315262941\n",
            "Dataset 8 model training, Epoch 56, Loss: 0.6194574118860616\n",
            "Dataset 8 model training, Epoch 57, Loss: 0.6241971726385539\n",
            "Dataset 8 model training, Epoch 58, Loss: 0.6144120569197123\n",
            "Dataset 8 model training, Epoch 59, Loss: 0.6186363079003839\n",
            "Dataset 8 model training, Epoch 60, Loss: 0.6050379418286701\n",
            "Dataset 8 model training, Epoch 61, Loss: 0.6164540712465376\n",
            "Dataset 8 model training, Epoch 62, Loss: 0.6112324917476449\n",
            "Dataset 8 model training, Epoch 63, Loss: 0.6124218074267342\n",
            "Dataset 8 model training, Epoch 64, Loss: 0.6117889853131852\n",
            "Dataset 8 model training, Epoch 65, Loss: 0.6180669581330063\n",
            "Dataset 8 model training, Epoch 66, Loss: 0.6033031074392715\n",
            "Dataset 8 model training, Epoch 67, Loss: 0.6111507763798605\n",
            "Dataset 8 model training, Epoch 68, Loss: 0.6113536277873404\n",
            "Dataset 8 model training, Epoch 69, Loss: 0.6096614289203747\n",
            "Dataset 8 model training, Epoch 70, Loss: 0.5983176217383186\n",
            "Dataset 8 model training, Epoch 71, Loss: 0.6016430836795961\n",
            "Dataset 8 model training, Epoch 72, Loss: 0.6094254537316777\n",
            "Dataset 8 model training, Epoch 73, Loss: 0.6022788256206768\n",
            "Dataset 8 model training, Epoch 74, Loss: 0.6080613528322053\n",
            "Dataset 8 model training, Epoch 75, Loss: 0.5962440767544228\n",
            "Dataset 8 model training, Epoch 76, Loss: 0.5931099509232797\n",
            "Dataset 8 model training, Epoch 77, Loss: 0.594499275188318\n",
            "Dataset 8 model training, Epoch 78, Loss: 0.6053171589870581\n",
            "Dataset 8 model training, Epoch 79, Loss: 0.5963082643563316\n",
            "Dataset 8 model training, Epoch 80, Loss: 0.6037595196058286\n",
            "Dataset 8 model training, Epoch 81, Loss: 0.6108064269459488\n",
            "Dataset 8 model training, Epoch 82, Loss: 0.6001530029629701\n",
            "Dataset 8 model training, Epoch 83, Loss: 0.591821018281399\n",
            "Dataset 8 model training, Epoch 84, Loss: 0.5853031671687261\n",
            "Dataset 8 model training, Epoch 85, Loss: 0.5790872697862203\n",
            "Dataset 8 model training, Epoch 86, Loss: 0.5787688925762304\n",
            "Dataset 8 model training, Epoch 87, Loss: 0.57789867336318\n",
            "Dataset 8 model training, Epoch 88, Loss: 0.5796908522212265\n",
            "Dataset 8 model training, Epoch 89, Loss: 0.5761975914039867\n",
            "Dataset 8 model training, Epoch 90, Loss: 0.5765572148281456\n",
            "Dataset 8 model training, Epoch 91, Loss: 0.5758673334681748\n",
            "Dataset 8 model training, Epoch 92, Loss: 0.5795633082821865\n",
            "Dataset 8 model training, Epoch 93, Loss: 0.5701235246738331\n",
            "Dataset 8 model training, Epoch 94, Loss: 0.5713326452962504\n",
            "Dataset 8 model training, Epoch 95, Loss: 0.5844722796206506\n",
            "Dataset 8 model training, Epoch 96, Loss: 0.5783396897299978\n",
            "Dataset 8 model training, Epoch 97, Loss: 0.5698140283558992\n",
            "Dataset 8 model training, Epoch 98, Loss: 0.5800272730372896\n",
            "Dataset 8 model training, Epoch 99, Loss: 0.5736517678171196\n",
            "Dataset 8 model training, Epoch 100, Loss: 0.5801277540674146\n",
            "Dataset 8 model training, Epoch 101, Loss: 0.5694393735603999\n",
            "Dataset 8 model training, Epoch 102, Loss: 0.574445637280509\n",
            "Dataset 8 model training, Epoch 103, Loss: 0.5713119708851679\n",
            "Dataset 8 model training, Epoch 104, Loss: 0.5751537946646645\n",
            "Dataset 8 model training, Epoch 105, Loss: 0.575456279836245\n",
            "Dataset 8 model training, Epoch 106, Loss: 0.5694063035833755\n",
            "Dataset 8 model training, Epoch 107, Loss: 0.5750091735948653\n",
            "Dataset 8 model training, Epoch 108, Loss: 0.5679383073877168\n",
            "Dataset 8 model training, Epoch 109, Loss: 0.5659391259987082\n",
            "Dataset 8 model training, Epoch 110, Loss: 0.5710931998371278\n",
            "Dataset 8 model training, Epoch 111, Loss: 0.569607394053632\n",
            "Dataset 8 model training, Epoch 112, Loss: 0.5579104609537444\n",
            "Dataset 8 model training, Epoch 113, Loss: 0.5598500608997857\n",
            "Dataset 8 model training, Epoch 114, Loss: 0.5656227599454406\n",
            "Dataset 8 model training, Epoch 115, Loss: 0.5687552608499591\n",
            "Dataset 8 model training, Epoch 116, Loss: 0.5606743851364059\n",
            "Dataset 8 model training, Epoch 117, Loss: 0.558521097138424\n",
            "Dataset 8 model training, Epoch 118, Loss: 0.5591882091240595\n",
            "Dataset 8 model training, Epoch 119, Loss: 0.5682561181535657\n",
            "Dataset 8 model training, Epoch 120, Loss: 0.5523345196407113\n",
            "Dataset 8 model training, Epoch 121, Loss: 0.5598897509926918\n",
            "Dataset 8 model training, Epoch 122, Loss: 0.5533903359166727\n",
            "Dataset 8 model training, Epoch 123, Loss: 0.5683872139693906\n",
            "Dataset 8 model training, Epoch 124, Loss: 0.5530927683283019\n",
            "Dataset 8 model training, Epoch 125, Loss: 0.5603789151114905\n",
            "Dataset 8 model training, Epoch 126, Loss: 0.5610267099918135\n",
            "Dataset 8 model training, Epoch 127, Loss: 0.5559921670680078\n",
            "Dataset 8 model training, Epoch 128, Loss: 0.5625805722787076\n",
            "Dataset 8 model training, Epoch 129, Loss: 0.550329829622435\n",
            "Dataset 8 model training, Epoch 130, Loss: 0.5541499591113737\n",
            "Dataset 8 model training, Epoch 131, Loss: 0.5586424103919292\n",
            "Dataset 8 model training, Epoch 132, Loss: 0.5532448405787449\n",
            "Dataset 8 model training, Epoch 133, Loss: 0.5527278170889656\n",
            "Dataset 8 model training, Epoch 134, Loss: 0.5614671039101261\n",
            "Dataset 8 model training, Epoch 135, Loss: 0.5582914770449567\n",
            "Dataset 8 model training, Epoch 136, Loss: 0.5551199182968012\n",
            "Dataset 8 model training, Epoch 137, Loss: 0.557949631806188\n",
            "Dataset 8 model training, Epoch 138, Loss: 0.5534269765719472\n",
            "Early stopping triggered for Dataset 8 model after epoch 139\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 9/15\n",
            "Dataset 9 model training, Epoch 1, Loss: 1.4385956901393524\n",
            "Dataset 9 model training, Epoch 2, Loss: 1.1812318008239955\n",
            "Dataset 9 model training, Epoch 3, Loss: 1.021114953168451\n",
            "Dataset 9 model training, Epoch 4, Loss: 0.9270446847562921\n",
            "Dataset 9 model training, Epoch 5, Loss: 0.8782722153892256\n",
            "Dataset 9 model training, Epoch 6, Loss: 0.8315452623040709\n",
            "Dataset 9 model training, Epoch 7, Loss: 0.8209132944067864\n",
            "Dataset 9 model training, Epoch 8, Loss: 0.7953243565885988\n",
            "Dataset 9 model training, Epoch 9, Loss: 0.7771121496207094\n",
            "Dataset 9 model training, Epoch 10, Loss: 0.7683545322450873\n",
            "Dataset 9 model training, Epoch 11, Loss: 0.7475580532256871\n",
            "Dataset 9 model training, Epoch 12, Loss: 0.7424746314956717\n",
            "Dataset 9 model training, Epoch 13, Loss: 0.7242043075496203\n",
            "Dataset 9 model training, Epoch 14, Loss: 0.7144463997181147\n",
            "Dataset 9 model training, Epoch 15, Loss: 0.7096124818063763\n",
            "Dataset 9 model training, Epoch 16, Loss: 0.6966876118150476\n",
            "Dataset 9 model training, Epoch 17, Loss: 0.686891009138055\n",
            "Dataset 9 model training, Epoch 18, Loss: 0.6683609648926617\n",
            "Dataset 9 model training, Epoch 19, Loss: 0.6673619665511666\n",
            "Dataset 9 model training, Epoch 20, Loss: 0.6529490323099372\n",
            "Dataset 9 model training, Epoch 21, Loss: 0.655425925777383\n",
            "Dataset 9 model training, Epoch 22, Loss: 0.6415155899443038\n",
            "Dataset 9 model training, Epoch 23, Loss: 0.6463157444784086\n",
            "Dataset 9 model training, Epoch 24, Loss: 0.6440376767965212\n",
            "Dataset 9 model training, Epoch 25, Loss: 0.6405852990199442\n",
            "Dataset 9 model training, Epoch 26, Loss: 0.6376059345186573\n",
            "Dataset 9 model training, Epoch 27, Loss: 0.6331409579270506\n",
            "Dataset 9 model training, Epoch 28, Loss: 0.6329650399211335\n",
            "Dataset 9 model training, Epoch 29, Loss: 0.6306693135467294\n",
            "Dataset 9 model training, Epoch 30, Loss: 0.6252120976170449\n",
            "Dataset 9 model training, Epoch 31, Loss: 0.6225877057199609\n",
            "Dataset 9 model training, Epoch 32, Loss: 0.6283573322508433\n",
            "Dataset 9 model training, Epoch 33, Loss: 0.6244338034358743\n",
            "Dataset 9 model training, Epoch 34, Loss: 0.6205654791364931\n",
            "Dataset 9 model training, Epoch 35, Loss: 0.6171320752330023\n",
            "Dataset 9 model training, Epoch 36, Loss: 0.617855211847449\n",
            "Dataset 9 model training, Epoch 37, Loss: 0.6108943342346035\n",
            "Dataset 9 model training, Epoch 38, Loss: 0.6185874555208911\n",
            "Dataset 9 model training, Epoch 39, Loss: 0.610625925945909\n",
            "Dataset 9 model training, Epoch 40, Loss: 0.6113152944878356\n",
            "Dataset 9 model training, Epoch 41, Loss: 0.6133194233048452\n",
            "Dataset 9 model training, Epoch 42, Loss: 0.6087122914725787\n",
            "Dataset 9 model training, Epoch 43, Loss: 0.6151485871778776\n",
            "Dataset 9 model training, Epoch 44, Loss: 0.6054078665089934\n",
            "Dataset 9 model training, Epoch 45, Loss: 0.6104280944556406\n",
            "Dataset 9 model training, Epoch 46, Loss: 0.6008531416523947\n",
            "Dataset 9 model training, Epoch 47, Loss: 0.5978922374444465\n",
            "Dataset 9 model training, Epoch 48, Loss: 0.6075450043155722\n",
            "Dataset 9 model training, Epoch 49, Loss: 0.6054469008151799\n",
            "Dataset 9 model training, Epoch 50, Loss: 0.601832487811781\n",
            "Dataset 9 model training, Epoch 51, Loss: 0.5998953866632017\n",
            "Dataset 9 model training, Epoch 52, Loss: 0.598345769390668\n",
            "Dataset 9 model training, Epoch 53, Loss: 0.5976716816833575\n",
            "Dataset 9 model training, Epoch 54, Loss: 0.6086744500757897\n",
            "Dataset 9 model training, Epoch 55, Loss: 0.5969341529967034\n",
            "Dataset 9 model training, Epoch 56, Loss: 0.60214323560669\n",
            "Dataset 9 model training, Epoch 57, Loss: 0.5968030574795318\n",
            "Dataset 9 model training, Epoch 58, Loss: 0.6052977567257947\n",
            "Dataset 9 model training, Epoch 59, Loss: 0.5952380287320647\n",
            "Dataset 9 model training, Epoch 60, Loss: 0.5994321926407618\n",
            "Dataset 9 model training, Epoch 61, Loss: 0.6070471457014345\n",
            "Dataset 9 model training, Epoch 62, Loss: 0.592478288363104\n",
            "Dataset 9 model training, Epoch 63, Loss: 0.5967686588633551\n",
            "Dataset 9 model training, Epoch 64, Loss: 0.5971651326303613\n",
            "Dataset 9 model training, Epoch 65, Loss: 0.602384987146887\n",
            "Dataset 9 model training, Epoch 66, Loss: 0.6000894226440011\n",
            "Dataset 9 model training, Epoch 67, Loss: 0.5934349318889722\n",
            "Dataset 9 model training, Epoch 68, Loss: 0.5991062985299385\n",
            "Dataset 9 model training, Epoch 69, Loss: 0.5839061247159357\n",
            "Dataset 9 model training, Epoch 70, Loss: 0.5786798053408322\n",
            "Dataset 9 model training, Epoch 71, Loss: 0.5787930278337166\n",
            "Dataset 9 model training, Epoch 72, Loss: 0.5788873981123102\n",
            "Dataset 9 model training, Epoch 73, Loss: 0.5803402078478304\n",
            "Dataset 9 model training, Epoch 74, Loss: 0.5823632752242154\n",
            "Dataset 9 model training, Epoch 75, Loss: 0.5777357249227288\n",
            "Dataset 9 model training, Epoch 76, Loss: 0.575743777498807\n",
            "Dataset 9 model training, Epoch 77, Loss: 0.5731984413649938\n",
            "Dataset 9 model training, Epoch 78, Loss: 0.570766980721526\n",
            "Dataset 9 model training, Epoch 79, Loss: 0.5786932623141432\n",
            "Dataset 9 model training, Epoch 80, Loss: 0.5705082040943511\n",
            "Dataset 9 model training, Epoch 81, Loss: 0.5707818179914396\n",
            "Dataset 9 model training, Epoch 82, Loss: 0.572432835420517\n",
            "Dataset 9 model training, Epoch 83, Loss: 0.5776420290747734\n",
            "Dataset 9 model training, Epoch 84, Loss: 0.5693351999945837\n",
            "Dataset 9 model training, Epoch 85, Loss: 0.5722193517913557\n",
            "Dataset 9 model training, Epoch 86, Loss: 0.5759763509443362\n",
            "Dataset 9 model training, Epoch 87, Loss: 0.5701845592423661\n",
            "Dataset 9 model training, Epoch 88, Loss: 0.570974176264789\n",
            "Dataset 9 model training, Epoch 89, Loss: 0.5755770088058628\n",
            "Dataset 9 model training, Epoch 90, Loss: 0.5738026113951042\n",
            "Dataset 9 model training, Epoch 91, Loss: 0.5668441630797844\n",
            "Dataset 9 model training, Epoch 92, Loss: 0.5591207473245385\n",
            "Dataset 9 model training, Epoch 93, Loss: 0.5617936533199598\n",
            "Dataset 9 model training, Epoch 94, Loss: 0.5687609976285124\n",
            "Dataset 9 model training, Epoch 95, Loss: 0.5592288907668362\n",
            "Dataset 9 model training, Epoch 96, Loss: 0.5649140381241498\n",
            "Dataset 9 model training, Epoch 97, Loss: 0.5558842479774396\n",
            "Dataset 9 model training, Epoch 98, Loss: 0.5556651947841252\n",
            "Dataset 9 model training, Epoch 99, Loss: 0.5591877545804194\n",
            "Dataset 9 model training, Epoch 100, Loss: 0.5599006116390228\n",
            "Dataset 9 model training, Epoch 101, Loss: 0.5632217967346923\n",
            "Dataset 9 model training, Epoch 102, Loss: 0.5566285215828517\n",
            "Dataset 9 model training, Epoch 103, Loss: 0.558266423336447\n",
            "Dataset 9 model training, Epoch 104, Loss: 0.5582154532001443\n",
            "Dataset 9 model training, Epoch 105, Loss: 0.5533054242395374\n",
            "Dataset 9 model training, Epoch 106, Loss: 0.554509943478728\n",
            "Dataset 9 model training, Epoch 107, Loss: 0.5474816689344302\n",
            "Dataset 9 model training, Epoch 108, Loss: 0.5499927038199282\n",
            "Dataset 9 model training, Epoch 109, Loss: 0.5502212474199191\n",
            "Dataset 9 model training, Epoch 110, Loss: 0.5477623823162627\n",
            "Dataset 9 model training, Epoch 111, Loss: 0.5479786420521671\n",
            "Dataset 9 model training, Epoch 112, Loss: 0.5444505665808508\n",
            "Dataset 9 model training, Epoch 113, Loss: 0.5474046985172245\n",
            "Dataset 9 model training, Epoch 114, Loss: 0.5467052733244961\n",
            "Dataset 9 model training, Epoch 115, Loss: 0.5535475615769216\n",
            "Dataset 9 model training, Epoch 116, Loss: 0.5418329453223372\n",
            "Dataset 9 model training, Epoch 117, Loss: 0.5502595674909957\n",
            "Dataset 9 model training, Epoch 118, Loss: 0.5488118802847928\n",
            "Dataset 9 model training, Epoch 119, Loss: 0.5447761104123233\n",
            "Dataset 9 model training, Epoch 120, Loss: 0.545891766678797\n",
            "Dataset 9 model training, Epoch 121, Loss: 0.5432403779601398\n",
            "Dataset 9 model training, Epoch 122, Loss: 0.539846785468598\n",
            "Dataset 9 model training, Epoch 123, Loss: 0.5386095885952858\n",
            "Dataset 9 model training, Epoch 124, Loss: 0.5378111937682922\n",
            "Dataset 9 model training, Epoch 125, Loss: 0.5429674231842773\n",
            "Dataset 9 model training, Epoch 126, Loss: 0.546693301772418\n",
            "Dataset 9 model training, Epoch 127, Loss: 0.5516024299272119\n",
            "Dataset 9 model training, Epoch 128, Loss: 0.5517824643278775\n",
            "Dataset 9 model training, Epoch 129, Loss: 0.5486387042966607\n",
            "Dataset 9 model training, Epoch 130, Loss: 0.5461728252368431\n",
            "Dataset 9 model training, Epoch 131, Loss: 0.5408532699085262\n",
            "Dataset 9 model training, Epoch 132, Loss: 0.5438497927907395\n",
            "Dataset 9 model training, Epoch 133, Loss: 0.536212976256462\n",
            "Dataset 9 model training, Epoch 134, Loss: 0.5374067356325176\n",
            "Dataset 9 model training, Epoch 135, Loss: 0.5465040076268862\n",
            "Dataset 9 model training, Epoch 136, Loss: 0.5372526715063068\n",
            "Dataset 9 model training, Epoch 137, Loss: 0.5435063018374247\n",
            "Dataset 9 model training, Epoch 138, Loss: 0.539453656910217\n",
            "Dataset 9 model training, Epoch 139, Loss: 0.5424498044056435\n",
            "Dataset 9 model training, Epoch 140, Loss: 0.5310983102615565\n",
            "Dataset 9 model training, Epoch 141, Loss: 0.5314920509922995\n",
            "Dataset 9 model training, Epoch 142, Loss: 0.5332673798276953\n",
            "Dataset 9 model training, Epoch 143, Loss: 0.5380971907344583\n",
            "Dataset 9 model training, Epoch 144, Loss: 0.5400570186033641\n",
            "Dataset 9 model training, Epoch 145, Loss: 0.5353478183076806\n",
            "Dataset 9 model training, Epoch 146, Loss: 0.5365644446382783\n",
            "Dataset 9 model training, Epoch 147, Loss: 0.5359130442550738\n",
            "Dataset 9 model training, Epoch 148, Loss: 0.5385006451851702\n",
            "Dataset 9 model training, Epoch 149, Loss: 0.5394584083393829\n",
            "Early stopping triggered for Dataset 9 model after epoch 150\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 10/15\n",
            "Dataset 10 model training, Epoch 1, Loss: 1.3816507709653754\n",
            "Dataset 10 model training, Epoch 2, Loss: 1.1317680901602696\n",
            "Dataset 10 model training, Epoch 3, Loss: 0.9810952975561744\n",
            "Dataset 10 model training, Epoch 4, Loss: 0.8901962069304366\n",
            "Dataset 10 model training, Epoch 5, Loss: 0.8319064500300508\n",
            "Dataset 10 model training, Epoch 6, Loss: 0.8005492114707043\n",
            "Dataset 10 model training, Epoch 7, Loss: 0.7786261137378844\n",
            "Dataset 10 model training, Epoch 8, Loss: 0.7591956967585965\n",
            "Dataset 10 model training, Epoch 9, Loss: 0.7482201774653635\n",
            "Dataset 10 model training, Epoch 10, Loss: 0.727995924259487\n",
            "Dataset 10 model training, Epoch 11, Loss: 0.7177196247012991\n",
            "Dataset 10 model training, Epoch 12, Loss: 0.7021473395197015\n",
            "Dataset 10 model training, Epoch 13, Loss: 0.6947573764543784\n",
            "Dataset 10 model training, Epoch 14, Loss: 0.6823117497720217\n",
            "Dataset 10 model training, Epoch 15, Loss: 0.668847421674352\n",
            "Dataset 10 model training, Epoch 16, Loss: 0.6517534404993057\n",
            "Dataset 10 model training, Epoch 17, Loss: 0.6435999513456696\n",
            "Dataset 10 model training, Epoch 18, Loss: 0.6430576333874151\n",
            "Dataset 10 model training, Epoch 19, Loss: 0.624735425961645\n",
            "Dataset 10 model training, Epoch 20, Loss: 0.6242485383623525\n",
            "Dataset 10 model training, Epoch 21, Loss: 0.6191495586382715\n",
            "Dataset 10 model training, Epoch 22, Loss: 0.6208572999427193\n",
            "Dataset 10 model training, Epoch 23, Loss: 0.6134567770518755\n",
            "Dataset 10 model training, Epoch 24, Loss: 0.6109527914147628\n",
            "Dataset 10 model training, Epoch 25, Loss: 0.6104324352192251\n",
            "Dataset 10 model training, Epoch 26, Loss: 0.6030503333007035\n",
            "Dataset 10 model training, Epoch 27, Loss: 0.6010040148700538\n",
            "Dataset 10 model training, Epoch 28, Loss: 0.6052647574167502\n",
            "Dataset 10 model training, Epoch 29, Loss: 0.6033521174992386\n",
            "Dataset 10 model training, Epoch 30, Loss: 0.6026624817597238\n",
            "Dataset 10 model training, Epoch 31, Loss: 0.6022149967519861\n",
            "Dataset 10 model training, Epoch 32, Loss: 0.5957160978332946\n",
            "Dataset 10 model training, Epoch 33, Loss: 0.5961110617377257\n",
            "Dataset 10 model training, Epoch 34, Loss: 0.5970671629827273\n",
            "Dataset 10 model training, Epoch 35, Loss: 0.5951670372956678\n",
            "Dataset 10 model training, Epoch 36, Loss: 0.5803899310137096\n",
            "Dataset 10 model training, Epoch 37, Loss: 0.5883767534243433\n",
            "Dataset 10 model training, Epoch 38, Loss: 0.5887649141644177\n",
            "Dataset 10 model training, Epoch 39, Loss: 0.5857046671995991\n",
            "Dataset 10 model training, Epoch 40, Loss: 0.5872027723020629\n",
            "Dataset 10 model training, Epoch 41, Loss: 0.5812743810054503\n",
            "Dataset 10 model training, Epoch 42, Loss: 0.5892613200764907\n",
            "Dataset 10 model training, Epoch 43, Loss: 0.5790995300600403\n",
            "Dataset 10 model training, Epoch 44, Loss: 0.561552923956984\n",
            "Dataset 10 model training, Epoch 45, Loss: 0.562510117104179\n",
            "Dataset 10 model training, Epoch 46, Loss: 0.5658960646312488\n",
            "Dataset 10 model training, Epoch 47, Loss: 0.5646201750557673\n",
            "Dataset 10 model training, Epoch 48, Loss: 0.5659616495433607\n",
            "Dataset 10 model training, Epoch 49, Loss: 0.5606155571968932\n",
            "Dataset 10 model training, Epoch 50, Loss: 0.5612842295514909\n",
            "Dataset 10 model training, Epoch 51, Loss: 0.5622395545636353\n",
            "Dataset 10 model training, Epoch 52, Loss: 0.5575428752130583\n",
            "Dataset 10 model training, Epoch 53, Loss: 0.5667633257414165\n",
            "Dataset 10 model training, Epoch 54, Loss: 0.5569397585564538\n",
            "Dataset 10 model training, Epoch 55, Loss: 0.5622778052562162\n",
            "Dataset 10 model training, Epoch 56, Loss: 0.5585558179177736\n",
            "Dataset 10 model training, Epoch 57, Loss: 0.5543539610348249\n",
            "Dataset 10 model training, Epoch 58, Loss: 0.5543832004462418\n",
            "Dataset 10 model training, Epoch 59, Loss: 0.5548422775770488\n",
            "Dataset 10 model training, Epoch 60, Loss: 0.5459194132371953\n",
            "Dataset 10 model training, Epoch 61, Loss: 0.5593604486631719\n",
            "Dataset 10 model training, Epoch 62, Loss: 0.5497768297791481\n",
            "Dataset 10 model training, Epoch 63, Loss: 0.5505825155659726\n",
            "Dataset 10 model training, Epoch 64, Loss: 0.5536592594887081\n",
            "Dataset 10 model training, Epoch 65, Loss: 0.55577273823713\n",
            "Dataset 10 model training, Epoch 66, Loss: 0.5489763152834616\n",
            "Dataset 10 model training, Epoch 67, Loss: 0.544321745438011\n",
            "Dataset 10 model training, Epoch 68, Loss: 0.538207040217362\n",
            "Dataset 10 model training, Epoch 69, Loss: 0.531301507040074\n",
            "Dataset 10 model training, Epoch 70, Loss: 0.540621913381313\n",
            "Dataset 10 model training, Epoch 71, Loss: 0.5284023704497438\n",
            "Dataset 10 model training, Epoch 72, Loss: 0.5404198899080879\n",
            "Dataset 10 model training, Epoch 73, Loss: 0.5388166755437851\n",
            "Dataset 10 model training, Epoch 74, Loss: 0.5257876297753108\n",
            "Dataset 10 model training, Epoch 75, Loss: 0.5287829000306757\n",
            "Dataset 10 model training, Epoch 76, Loss: 0.5310762001103476\n",
            "Dataset 10 model training, Epoch 77, Loss: 0.5329982269751398\n",
            "Dataset 10 model training, Epoch 78, Loss: 0.5299863248671356\n",
            "Dataset 10 model training, Epoch 79, Loss: 0.53720385483221\n",
            "Dataset 10 model training, Epoch 80, Loss: 0.5279528259446746\n",
            "Dataset 10 model training, Epoch 81, Loss: 0.5257946558688816\n",
            "Dataset 10 model training, Epoch 82, Loss: 0.5153090487185278\n",
            "Dataset 10 model training, Epoch 83, Loss: 0.5168291332298204\n",
            "Dataset 10 model training, Epoch 84, Loss: 0.5169857971762356\n",
            "Dataset 10 model training, Epoch 85, Loss: 0.5212998190208485\n",
            "Dataset 10 model training, Epoch 86, Loss: 0.5138492490115919\n",
            "Dataset 10 model training, Epoch 87, Loss: 0.531578950191799\n",
            "Dataset 10 model training, Epoch 88, Loss: 0.5156550387802877\n",
            "Dataset 10 model training, Epoch 89, Loss: 0.5142228017119985\n",
            "Dataset 10 model training, Epoch 90, Loss: 0.5111842369170565\n",
            "Dataset 10 model training, Epoch 91, Loss: 0.5165408445816291\n",
            "Dataset 10 model training, Epoch 92, Loss: 0.525220771761317\n",
            "Dataset 10 model training, Epoch 93, Loss: 0.5033413338425913\n",
            "Dataset 10 model training, Epoch 94, Loss: 0.5196414595763934\n",
            "Dataset 10 model training, Epoch 95, Loss: 0.5171369927886286\n",
            "Dataset 10 model training, Epoch 96, Loss: 0.5188264799745459\n",
            "Dataset 10 model training, Epoch 97, Loss: 0.5128457932487914\n",
            "Dataset 10 model training, Epoch 98, Loss: 0.5197375097164982\n",
            "Dataset 10 model training, Epoch 99, Loss: 0.5170209415649113\n",
            "Dataset 10 model training, Epoch 100, Loss: 0.5132645562683281\n",
            "Dataset 10 model training, Epoch 101, Loss: 0.5130158411829095\n",
            "Dataset 10 model training, Epoch 102, Loss: 0.5106916015869692\n",
            "Early stopping triggered for Dataset 10 model after epoch 103\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 11/15\n",
            "Dataset 11 model training, Epoch 1, Loss: 1.285055389834775\n",
            "Dataset 11 model training, Epoch 2, Loss: 1.0190531110597982\n",
            "Dataset 11 model training, Epoch 3, Loss: 0.8714957688417699\n",
            "Dataset 11 model training, Epoch 4, Loss: 0.7929308956695927\n",
            "Dataset 11 model training, Epoch 5, Loss: 0.7489138705035051\n",
            "Dataset 11 model training, Epoch 6, Loss: 0.7080564668609036\n",
            "Dataset 11 model training, Epoch 7, Loss: 0.6976658060318894\n",
            "Dataset 11 model training, Epoch 8, Loss: 0.6735043502930138\n",
            "Dataset 11 model training, Epoch 9, Loss: 0.6590943524820937\n",
            "Dataset 11 model training, Epoch 10, Loss: 0.6451152561025487\n",
            "Dataset 11 model training, Epoch 11, Loss: 0.645271208965116\n",
            "Dataset 11 model training, Epoch 12, Loss: 0.6266599034683572\n",
            "Dataset 11 model training, Epoch 13, Loss: 0.6136764559066958\n",
            "Dataset 11 model training, Epoch 14, Loss: 0.6095097242958016\n",
            "Dataset 11 model training, Epoch 15, Loss: 0.5878214958227344\n",
            "Dataset 11 model training, Epoch 16, Loss: 0.5871820271843009\n",
            "Dataset 11 model training, Epoch 17, Loss: 0.5777102272129722\n",
            "Dataset 11 model training, Epoch 18, Loss: 0.5694104234377543\n",
            "Dataset 11 model training, Epoch 19, Loss: 0.554761461292704\n",
            "Dataset 11 model training, Epoch 20, Loss: 0.5536142037146621\n",
            "Dataset 11 model training, Epoch 21, Loss: 0.5487961437967088\n",
            "Dataset 11 model training, Epoch 22, Loss: 0.5485774653239383\n",
            "Dataset 11 model training, Epoch 23, Loss: 0.5409588942097293\n",
            "Dataset 11 model training, Epoch 24, Loss: 0.5419979105807013\n",
            "Dataset 11 model training, Epoch 25, Loss: 0.5455536362197664\n",
            "Dataset 11 model training, Epoch 26, Loss: 0.5432044050345818\n",
            "Dataset 11 model training, Epoch 27, Loss: 0.5328236083603568\n",
            "Dataset 11 model training, Epoch 28, Loss: 0.5326020620349381\n",
            "Dataset 11 model training, Epoch 29, Loss: 0.5290430498619875\n",
            "Dataset 11 model training, Epoch 30, Loss: 0.5325025624285141\n",
            "Dataset 11 model training, Epoch 31, Loss: 0.5311726319293181\n",
            "Dataset 11 model training, Epoch 32, Loss: 0.5243836804810498\n",
            "Dataset 11 model training, Epoch 33, Loss: 0.5209505760835277\n",
            "Dataset 11 model training, Epoch 34, Loss: 0.5203336564203104\n",
            "Dataset 11 model training, Epoch 35, Loss: 0.5229056363718377\n",
            "Dataset 11 model training, Epoch 36, Loss: 0.5197270462910334\n",
            "Dataset 11 model training, Epoch 37, Loss: 0.5156024990396367\n",
            "Dataset 11 model training, Epoch 38, Loss: 0.5140069692085186\n",
            "Dataset 11 model training, Epoch 39, Loss: 0.5115754467745622\n",
            "Dataset 11 model training, Epoch 40, Loss: 0.506741126999259\n",
            "Dataset 11 model training, Epoch 41, Loss: 0.5069891388217608\n",
            "Dataset 11 model training, Epoch 42, Loss: 0.5127192793620957\n",
            "Dataset 11 model training, Epoch 43, Loss: 0.5109272030078702\n",
            "Dataset 11 model training, Epoch 44, Loss: 0.5028290653394328\n",
            "Dataset 11 model training, Epoch 45, Loss: 0.5059003662317991\n",
            "Dataset 11 model training, Epoch 46, Loss: 0.5096090775397089\n",
            "Dataset 11 model training, Epoch 47, Loss: 0.5046322060128053\n",
            "Dataset 11 model training, Epoch 48, Loss: 0.5072874031547043\n",
            "Dataset 11 model training, Epoch 49, Loss: 0.5081329776181115\n",
            "Dataset 11 model training, Epoch 50, Loss: 0.5038213773320118\n",
            "Dataset 11 model training, Epoch 51, Loss: 0.496803214152654\n",
            "Dataset 11 model training, Epoch 52, Loss: 0.49198661516937947\n",
            "Dataset 11 model training, Epoch 53, Loss: 0.4838861593355735\n",
            "Dataset 11 model training, Epoch 54, Loss: 0.4775203930007087\n",
            "Dataset 11 model training, Epoch 55, Loss: 0.4895193392617835\n",
            "Dataset 11 model training, Epoch 56, Loss: 0.48401658112804097\n",
            "Dataset 11 model training, Epoch 57, Loss: 0.4869407233264711\n",
            "Dataset 11 model training, Epoch 58, Loss: 0.4815986135767566\n",
            "Dataset 11 model training, Epoch 59, Loss: 0.4844560083001852\n",
            "Dataset 11 model training, Epoch 60, Loss: 0.48314864974882865\n",
            "Dataset 11 model training, Epoch 61, Loss: 0.47222088649868965\n",
            "Dataset 11 model training, Epoch 62, Loss: 0.470335199808081\n",
            "Dataset 11 model training, Epoch 63, Loss: 0.46867583071192104\n",
            "Dataset 11 model training, Epoch 64, Loss: 0.46107751814027625\n",
            "Dataset 11 model training, Epoch 65, Loss: 0.46909699713190395\n",
            "Dataset 11 model training, Epoch 66, Loss: 0.46801072048644227\n",
            "Dataset 11 model training, Epoch 67, Loss: 0.46426375168893075\n",
            "Dataset 11 model training, Epoch 68, Loss: 0.46187226432893014\n",
            "Dataset 11 model training, Epoch 69, Loss: 0.4666568587223689\n",
            "Dataset 11 model training, Epoch 70, Loss: 0.46049775804082554\n",
            "Dataset 11 model training, Epoch 71, Loss: 0.4598267852432198\n",
            "Dataset 11 model training, Epoch 72, Loss: 0.4657748641653193\n",
            "Dataset 11 model training, Epoch 73, Loss: 0.4581105417261521\n",
            "Dataset 11 model training, Epoch 74, Loss: 0.45957642669479054\n",
            "Dataset 11 model training, Epoch 75, Loss: 0.4627647952487071\n",
            "Dataset 11 model training, Epoch 76, Loss: 0.46233559399843216\n",
            "Dataset 11 model training, Epoch 77, Loss: 0.4607357333103816\n",
            "Dataset 11 model training, Epoch 78, Loss: 0.4572022938066059\n",
            "Dataset 11 model training, Epoch 79, Loss: 0.4603134780708287\n",
            "Dataset 11 model training, Epoch 80, Loss: 0.4640818482471837\n",
            "Dataset 11 model training, Epoch 81, Loss: 0.4596387623912758\n",
            "Dataset 11 model training, Epoch 82, Loss: 0.46488107326957917\n",
            "Dataset 11 model training, Epoch 83, Loss: 0.46007514455252224\n",
            "Dataset 11 model training, Epoch 84, Loss: 0.4561033190952407\n",
            "Dataset 11 model training, Epoch 85, Loss: 0.4466428477317095\n",
            "Dataset 11 model training, Epoch 86, Loss: 0.4590426054265764\n",
            "Dataset 11 model training, Epoch 87, Loss: 0.4606168125238683\n",
            "Dataset 11 model training, Epoch 88, Loss: 0.4599158805277612\n",
            "Dataset 11 model training, Epoch 89, Loss: 0.45399534495340454\n",
            "Dataset 11 model training, Epoch 90, Loss: 0.4555024467408657\n",
            "Dataset 11 model training, Epoch 91, Loss: 0.4542458231250445\n",
            "Dataset 11 model training, Epoch 92, Loss: 0.4451116180668275\n",
            "Dataset 11 model training, Epoch 93, Loss: 0.44820338984330493\n",
            "Dataset 11 model training, Epoch 94, Loss: 0.44514352621303666\n",
            "Dataset 11 model training, Epoch 95, Loss: 0.4446230216158761\n",
            "Dataset 11 model training, Epoch 96, Loss: 0.44333062341643703\n",
            "Dataset 11 model training, Epoch 97, Loss: 0.4368400180505382\n",
            "Dataset 11 model training, Epoch 98, Loss: 0.44522229002581704\n",
            "Dataset 11 model training, Epoch 99, Loss: 0.44201086399455863\n",
            "Dataset 11 model training, Epoch 100, Loss: 0.44578908218277824\n",
            "Dataset 11 model training, Epoch 101, Loss: 0.43811252920164\n",
            "Dataset 11 model training, Epoch 102, Loss: 0.43950590801735717\n",
            "Dataset 11 model training, Epoch 103, Loss: 0.4359916943228907\n",
            "Dataset 11 model training, Epoch 104, Loss: 0.4425559954510795\n",
            "Dataset 11 model training, Epoch 105, Loss: 0.44124530835284126\n",
            "Dataset 11 model training, Epoch 106, Loss: 0.4428518232372072\n",
            "Dataset 11 model training, Epoch 107, Loss: 0.4437248479160998\n",
            "Dataset 11 model training, Epoch 108, Loss: 0.44602515424291295\n",
            "Dataset 11 model training, Epoch 109, Loss: 0.4420362131463157\n",
            "Dataset 11 model training, Epoch 110, Loss: 0.4325586696051889\n",
            "Dataset 11 model training, Epoch 111, Loss: 0.4364067783786191\n",
            "Dataset 11 model training, Epoch 112, Loss: 0.4384850511948268\n",
            "Dataset 11 model training, Epoch 113, Loss: 0.4310057715823253\n",
            "Dataset 11 model training, Epoch 114, Loss: 0.42987966785828274\n",
            "Dataset 11 model training, Epoch 115, Loss: 0.43473036628630424\n",
            "Dataset 11 model training, Epoch 116, Loss: 0.43426801988648045\n",
            "Dataset 11 model training, Epoch 117, Loss: 0.43366304226219654\n",
            "Dataset 11 model training, Epoch 118, Loss: 0.43056568110154736\n",
            "Dataset 11 model training, Epoch 119, Loss: 0.42914124640325707\n",
            "Dataset 11 model training, Epoch 120, Loss: 0.42976779946022564\n",
            "Dataset 11 model training, Epoch 121, Loss: 0.4382796171638701\n",
            "Dataset 11 model training, Epoch 122, Loss: 0.4332032576203346\n",
            "Dataset 11 model training, Epoch 123, Loss: 0.43415850897630054\n",
            "Dataset 11 model training, Epoch 124, Loss: 0.4224526911146111\n",
            "Dataset 11 model training, Epoch 125, Loss: 0.4273628224101331\n",
            "Dataset 11 model training, Epoch 126, Loss: 0.4254376542650991\n",
            "Dataset 11 model training, Epoch 127, Loss: 0.42554251787563163\n",
            "Dataset 11 model training, Epoch 128, Loss: 0.41843135096132755\n",
            "Dataset 11 model training, Epoch 129, Loss: 0.4268029710898797\n",
            "Dataset 11 model training, Epoch 130, Loss: 0.42977279693716103\n",
            "Dataset 11 model training, Epoch 131, Loss: 0.4302531664984094\n",
            "Dataset 11 model training, Epoch 132, Loss: 0.43124947759012383\n",
            "Dataset 11 model training, Epoch 133, Loss: 0.4272684154825078\n",
            "Dataset 11 model training, Epoch 134, Loss: 0.42415862406293553\n",
            "Dataset 11 model training, Epoch 135, Loss: 0.4299749715460671\n",
            "Dataset 11 model training, Epoch 136, Loss: 0.4284738608532482\n",
            "Dataset 11 model training, Epoch 137, Loss: 0.4264638773683045\n",
            "Early stopping triggered for Dataset 11 model after epoch 138\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 12/15\n",
            "Dataset 12 model training, Epoch 1, Loss: 1.4261092476441826\n",
            "Dataset 12 model training, Epoch 2, Loss: 1.1885004320614774\n",
            "Dataset 12 model training, Epoch 3, Loss: 1.0505372096954937\n",
            "Dataset 12 model training, Epoch 4, Loss: 0.9475609534223315\n",
            "Dataset 12 model training, Epoch 5, Loss: 0.8901355883605043\n",
            "Dataset 12 model training, Epoch 6, Loss: 0.8470185533375807\n",
            "Dataset 12 model training, Epoch 7, Loss: 0.81597116505596\n",
            "Dataset 12 model training, Epoch 8, Loss: 0.7944380347157868\n",
            "Dataset 12 model training, Epoch 9, Loss: 0.7806698537208665\n",
            "Dataset 12 model training, Epoch 10, Loss: 0.7700984511576908\n",
            "Dataset 12 model training, Epoch 11, Loss: 0.753554578398315\n",
            "Dataset 12 model training, Epoch 12, Loss: 0.7403239049542119\n",
            "Dataset 12 model training, Epoch 13, Loss: 0.7219181228691424\n",
            "Dataset 12 model training, Epoch 14, Loss: 0.7111258477392332\n",
            "Dataset 12 model training, Epoch 15, Loss: 0.7001513236425292\n",
            "Dataset 12 model training, Epoch 16, Loss: 0.6847155471922646\n",
            "Dataset 12 model training, Epoch 17, Loss: 0.6791116654033392\n",
            "Dataset 12 model training, Epoch 18, Loss: 0.6658093782377915\n",
            "Dataset 12 model training, Epoch 19, Loss: 0.6575922921929561\n",
            "Dataset 12 model training, Epoch 20, Loss: 0.6505012348504133\n",
            "Dataset 12 model training, Epoch 21, Loss: 0.6388936714387276\n",
            "Dataset 12 model training, Epoch 22, Loss: 0.6398546582376453\n",
            "Dataset 12 model training, Epoch 23, Loss: 0.6389407996140736\n",
            "Dataset 12 model training, Epoch 24, Loss: 0.6318849425920299\n",
            "Dataset 12 model training, Epoch 25, Loss: 0.6296898527464396\n",
            "Dataset 12 model training, Epoch 26, Loss: 0.6262086090487493\n",
            "Dataset 12 model training, Epoch 27, Loss: 0.624323704922703\n",
            "Dataset 12 model training, Epoch 28, Loss: 0.624567020107323\n",
            "Dataset 12 model training, Epoch 29, Loss: 0.6174640080458681\n",
            "Dataset 12 model training, Epoch 30, Loss: 0.6211965596172172\n",
            "Dataset 12 model training, Epoch 31, Loss: 0.6208258938621467\n",
            "Dataset 12 model training, Epoch 32, Loss: 0.6169651119222104\n",
            "Dataset 12 model training, Epoch 33, Loss: 0.6188237075234803\n",
            "Dataset 12 model training, Epoch 34, Loss: 0.6202110293465601\n",
            "Dataset 12 model training, Epoch 35, Loss: 0.6170289484967648\n",
            "Dataset 12 model training, Epoch 36, Loss: 0.6180038227581642\n",
            "Dataset 12 model training, Epoch 37, Loss: 0.6172354853069278\n",
            "Dataset 12 model training, Epoch 38, Loss: 0.6164274687918139\n",
            "Dataset 12 model training, Epoch 39, Loss: 0.612957349125768\n",
            "Dataset 12 model training, Epoch 40, Loss: 0.6122938495286754\n",
            "Dataset 12 model training, Epoch 41, Loss: 0.6089381445461596\n",
            "Dataset 12 model training, Epoch 42, Loss: 0.6048268721976751\n",
            "Dataset 12 model training, Epoch 43, Loss: 0.6106083560997332\n",
            "Dataset 12 model training, Epoch 44, Loss: 0.6109224228372037\n",
            "Dataset 12 model training, Epoch 45, Loss: 0.6142113615929241\n",
            "Dataset 12 model training, Epoch 46, Loss: 0.6133604236471821\n",
            "Dataset 12 model training, Epoch 47, Loss: 0.6064214462965307\n",
            "Dataset 12 model training, Epoch 48, Loss: 0.6056128906112321\n",
            "Dataset 12 model training, Epoch 49, Loss: 0.591757305281263\n",
            "Dataset 12 model training, Epoch 50, Loss: 0.5928874045190676\n",
            "Dataset 12 model training, Epoch 51, Loss: 0.5930428278278297\n",
            "Dataset 12 model training, Epoch 52, Loss: 0.5954657856007697\n",
            "Dataset 12 model training, Epoch 53, Loss: 0.5876265367571737\n",
            "Dataset 12 model training, Epoch 54, Loss: 0.5927544565687717\n",
            "Dataset 12 model training, Epoch 55, Loss: 0.5899570288372712\n",
            "Dataset 12 model training, Epoch 56, Loss: 0.5898321306621525\n",
            "Dataset 12 model training, Epoch 57, Loss: 0.5905173832262066\n",
            "Dataset 12 model training, Epoch 58, Loss: 0.5859084523899455\n",
            "Dataset 12 model training, Epoch 59, Loss: 0.5809563073054166\n",
            "Dataset 12 model training, Epoch 60, Loss: 0.5856907040300504\n",
            "Dataset 12 model training, Epoch 61, Loss: 0.5821866084572295\n",
            "Dataset 12 model training, Epoch 62, Loss: 0.5858055489583754\n",
            "Dataset 12 model training, Epoch 63, Loss: 0.5873605871284512\n",
            "Dataset 12 model training, Epoch 64, Loss: 0.5887034134545797\n",
            "Dataset 12 model training, Epoch 65, Loss: 0.5871810346422061\n",
            "Dataset 12 model training, Epoch 66, Loss: 0.5825259227987746\n",
            "Dataset 12 model training, Epoch 67, Loss: 0.580104369092995\n",
            "Dataset 12 model training, Epoch 68, Loss: 0.5802293468109319\n",
            "Dataset 12 model training, Epoch 69, Loss: 0.5782975432738452\n",
            "Dataset 12 model training, Epoch 70, Loss: 0.5747859051949541\n",
            "Dataset 12 model training, Epoch 71, Loss: 0.5710671302718175\n",
            "Dataset 12 model training, Epoch 72, Loss: 0.5724580967930001\n",
            "Dataset 12 model training, Epoch 73, Loss: 0.5703589979191901\n",
            "Dataset 12 model training, Epoch 74, Loss: 0.5751399716860811\n",
            "Dataset 12 model training, Epoch 75, Loss: 0.5723416607984355\n",
            "Dataset 12 model training, Epoch 76, Loss: 0.5706942186389171\n",
            "Dataset 12 model training, Epoch 77, Loss: 0.5716761757790203\n",
            "Dataset 12 model training, Epoch 78, Loss: 0.5725912166313386\n",
            "Dataset 12 model training, Epoch 79, Loss: 0.5744646134930598\n",
            "Dataset 12 model training, Epoch 80, Loss: 0.564994581568409\n",
            "Dataset 12 model training, Epoch 81, Loss: 0.5639722330469481\n",
            "Dataset 12 model training, Epoch 82, Loss: 0.5671920576985453\n",
            "Dataset 12 model training, Epoch 83, Loss: 0.5649327940084565\n",
            "Dataset 12 model training, Epoch 84, Loss: 0.5621319744788426\n",
            "Dataset 12 model training, Epoch 85, Loss: 0.5685426913936373\n",
            "Dataset 12 model training, Epoch 86, Loss: 0.5626320398189653\n",
            "Dataset 12 model training, Epoch 87, Loss: 0.5668374755432908\n",
            "Dataset 12 model training, Epoch 88, Loss: 0.5641073303323396\n",
            "Dataset 12 model training, Epoch 89, Loss: 0.5635273458672242\n",
            "Dataset 12 model training, Epoch 90, Loss: 0.56726632487606\n",
            "Dataset 12 model training, Epoch 91, Loss: 0.5543114668886426\n",
            "Dataset 12 model training, Epoch 92, Loss: 0.5564009184148949\n",
            "Dataset 12 model training, Epoch 93, Loss: 0.5566613844162981\n",
            "Dataset 12 model training, Epoch 94, Loss: 0.5570792115070451\n",
            "Dataset 12 model training, Epoch 95, Loss: 0.552710974510287\n",
            "Dataset 12 model training, Epoch 96, Loss: 0.5602726287825007\n",
            "Dataset 12 model training, Epoch 97, Loss: 0.5592908999869521\n",
            "Dataset 12 model training, Epoch 98, Loss: 0.555325264452209\n",
            "Dataset 12 model training, Epoch 99, Loss: 0.5578318689490708\n",
            "Dataset 12 model training, Epoch 100, Loss: 0.554537574170341\n",
            "Dataset 12 model training, Epoch 101, Loss: 0.5589240474600188\n",
            "Dataset 12 model training, Epoch 102, Loss: 0.5596502068596827\n",
            "Dataset 12 model training, Epoch 103, Loss: 0.5513436781688476\n",
            "Dataset 12 model training, Epoch 104, Loss: 0.5548606019624522\n",
            "Dataset 12 model training, Epoch 105, Loss: 0.5530284477371565\n",
            "Dataset 12 model training, Epoch 106, Loss: 0.5542953457211105\n",
            "Dataset 12 model training, Epoch 107, Loss: 0.5547207822682152\n",
            "Dataset 12 model training, Epoch 108, Loss: 0.5519493098410082\n",
            "Dataset 12 model training, Epoch 109, Loss: 0.5553675973919076\n",
            "Dataset 12 model training, Epoch 110, Loss: 0.5547404532701197\n",
            "Dataset 12 model training, Epoch 111, Loss: 0.5509232456835222\n",
            "Dataset 12 model training, Epoch 112, Loss: 0.5529207780327595\n",
            "Dataset 12 model training, Epoch 113, Loss: 0.5542113197521424\n",
            "Dataset 12 model training, Epoch 114, Loss: 0.5516974369824772\n",
            "Dataset 12 model training, Epoch 115, Loss: 0.5499790193329395\n",
            "Dataset 12 model training, Epoch 116, Loss: 0.5503471025698622\n",
            "Dataset 12 model training, Epoch 117, Loss: 0.5550061340483141\n",
            "Dataset 12 model training, Epoch 118, Loss: 0.5504874540886409\n",
            "Dataset 12 model training, Epoch 119, Loss: 0.5561761087934736\n",
            "Dataset 12 model training, Epoch 120, Loss: 0.5582810071572452\n",
            "Dataset 12 model training, Epoch 121, Loss: 0.55975160443447\n",
            "Dataset 12 model training, Epoch 122, Loss: 0.5560882486088176\n",
            "Dataset 12 model training, Epoch 123, Loss: 0.5530730769248076\n",
            "Dataset 12 model training, Epoch 124, Loss: 0.5534704596223966\n",
            "Dataset 12 model training, Epoch 125, Loss: 0.5488781954201174\n",
            "Dataset 12 model training, Epoch 126, Loss: 0.5529529028375384\n",
            "Dataset 12 model training, Epoch 127, Loss: 0.5546307593164309\n",
            "Dataset 12 model training, Epoch 128, Loss: 0.5506486153938401\n",
            "Dataset 12 model training, Epoch 129, Loss: 0.5542201215112713\n",
            "Dataset 12 model training, Epoch 130, Loss: 0.5479418960675387\n",
            "Dataset 12 model training, Epoch 131, Loss: 0.5547220224226025\n",
            "Dataset 12 model training, Epoch 132, Loss: 0.5487302730620747\n",
            "Dataset 12 model training, Epoch 133, Loss: 0.5559377067945372\n",
            "Dataset 12 model training, Epoch 134, Loss: 0.5461220787444585\n",
            "Dataset 12 model training, Epoch 135, Loss: 0.5537527421830406\n",
            "Dataset 12 model training, Epoch 136, Loss: 0.5550492800457377\n",
            "Dataset 12 model training, Epoch 137, Loss: 0.5497917184527491\n",
            "Dataset 12 model training, Epoch 138, Loss: 0.5554443902113069\n",
            "Dataset 12 model training, Epoch 139, Loss: 0.5481503544978692\n",
            "Dataset 12 model training, Epoch 140, Loss: 0.5489423614992223\n",
            "Dataset 12 model training, Epoch 141, Loss: 0.5497359923073943\n",
            "Dataset 12 model training, Epoch 142, Loss: 0.552934339348699\n",
            "Dataset 12 model training, Epoch 143, Loss: 0.5540565216625241\n",
            "Early stopping triggered for Dataset 12 model after epoch 144\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 13/15\n",
            "Dataset 13 model training, Epoch 1, Loss: 1.4524312751745088\n",
            "Dataset 13 model training, Epoch 2, Loss: 1.1715831468307893\n",
            "Dataset 13 model training, Epoch 3, Loss: 1.0266648382922403\n",
            "Dataset 13 model training, Epoch 4, Loss: 0.9403447282859703\n",
            "Dataset 13 model training, Epoch 5, Loss: 0.8900276699097328\n",
            "Dataset 13 model training, Epoch 6, Loss: 0.8547549224367329\n",
            "Dataset 13 model training, Epoch 7, Loss: 0.8261344717218985\n",
            "Dataset 13 model training, Epoch 8, Loss: 0.814692593088337\n",
            "Dataset 13 model training, Epoch 9, Loss: 0.7980051379577786\n",
            "Dataset 13 model training, Epoch 10, Loss: 0.7783300545480516\n",
            "Dataset 13 model training, Epoch 11, Loss: 0.7656008733643426\n",
            "Dataset 13 model training, Epoch 12, Loss: 0.7476010061556997\n",
            "Dataset 13 model training, Epoch 13, Loss: 0.7418403987791024\n",
            "Dataset 13 model training, Epoch 14, Loss: 0.7200334446102965\n",
            "Dataset 13 model training, Epoch 15, Loss: 0.7095670365040598\n",
            "Dataset 13 model training, Epoch 16, Loss: 0.7052082707679349\n",
            "Dataset 13 model training, Epoch 17, Loss: 0.6922096828230067\n",
            "Dataset 13 model training, Epoch 18, Loss: 0.6791585485140482\n",
            "Dataset 13 model training, Epoch 19, Loss: 0.6777727600016625\n",
            "Dataset 13 model training, Epoch 20, Loss: 0.6777754687016306\n",
            "Dataset 13 model training, Epoch 21, Loss: 0.6638062867074231\n",
            "Dataset 13 model training, Epoch 22, Loss: 0.6590993712150973\n",
            "Dataset 13 model training, Epoch 23, Loss: 0.6507346111964556\n",
            "Dataset 13 model training, Epoch 24, Loss: 0.6535284554257113\n",
            "Dataset 13 model training, Epoch 25, Loss: 0.6520905716746461\n",
            "Dataset 13 model training, Epoch 26, Loss: 0.646060866468093\n",
            "Dataset 13 model training, Epoch 27, Loss: 0.6503857514437508\n",
            "Dataset 13 model training, Epoch 28, Loss: 0.6408017112149132\n",
            "Dataset 13 model training, Epoch 29, Loss: 0.6452153424421946\n",
            "Dataset 13 model training, Epoch 30, Loss: 0.649176668497472\n",
            "Dataset 13 model training, Epoch 31, Loss: 0.6406304189582276\n",
            "Dataset 13 model training, Epoch 32, Loss: 0.6341822614856795\n",
            "Dataset 13 model training, Epoch 33, Loss: 0.6456172289022433\n",
            "Dataset 13 model training, Epoch 34, Loss: 0.6356683451365801\n",
            "Dataset 13 model training, Epoch 35, Loss: 0.6357653733172448\n",
            "Dataset 13 model training, Epoch 36, Loss: 0.6412410128350351\n",
            "Dataset 13 model training, Epoch 37, Loss: 0.6270218742828743\n",
            "Dataset 13 model training, Epoch 38, Loss: 0.6404435017140083\n",
            "Dataset 13 model training, Epoch 39, Loss: 0.6280910251966489\n",
            "Dataset 13 model training, Epoch 40, Loss: 0.6248282396715451\n",
            "Dataset 13 model training, Epoch 41, Loss: 0.6305806936781391\n",
            "Dataset 13 model training, Epoch 42, Loss: 0.6313298547189999\n",
            "Dataset 13 model training, Epoch 43, Loss: 0.6317975098401113\n",
            "Dataset 13 model training, Epoch 44, Loss: 0.6248253626761093\n",
            "Dataset 13 model training, Epoch 45, Loss: 0.6234876755016302\n",
            "Dataset 13 model training, Epoch 46, Loss: 0.6275564681470783\n",
            "Dataset 13 model training, Epoch 47, Loss: 0.6272396877700207\n",
            "Dataset 13 model training, Epoch 48, Loss: 0.6241857806841532\n",
            "Dataset 13 model training, Epoch 49, Loss: 0.6274360564409518\n",
            "Dataset 13 model training, Epoch 50, Loss: 0.6277233467382544\n",
            "Dataset 13 model training, Epoch 51, Loss: 0.6250461332159105\n",
            "Dataset 13 model training, Epoch 52, Loss: 0.6116298957198274\n",
            "Dataset 13 model training, Epoch 53, Loss: 0.6041355080464307\n",
            "Dataset 13 model training, Epoch 54, Loss: 0.6000334114031075\n",
            "Dataset 13 model training, Epoch 55, Loss: 0.5999663308555004\n",
            "Dataset 13 model training, Epoch 56, Loss: 0.6049690443316317\n",
            "Dataset 13 model training, Epoch 57, Loss: 0.5982358712776035\n",
            "Dataset 13 model training, Epoch 58, Loss: 0.5952110983967002\n",
            "Dataset 13 model training, Epoch 59, Loss: 0.593890007414849\n",
            "Dataset 13 model training, Epoch 60, Loss: 0.5946187739278755\n",
            "Dataset 13 model training, Epoch 61, Loss: 0.5941827351750891\n",
            "Dataset 13 model training, Epoch 62, Loss: 0.5856654112245522\n",
            "Dataset 13 model training, Epoch 63, Loss: 0.5960919019443537\n",
            "Dataset 13 model training, Epoch 64, Loss: 0.5921415986578449\n",
            "Dataset 13 model training, Epoch 65, Loss: 0.6004933350616031\n",
            "Dataset 13 model training, Epoch 66, Loss: 0.5879306434805877\n",
            "Dataset 13 model training, Epoch 67, Loss: 0.5898669480887893\n",
            "Dataset 13 model training, Epoch 68, Loss: 0.5888083271341387\n",
            "Dataset 13 model training, Epoch 69, Loss: 0.5799788006770066\n",
            "Dataset 13 model training, Epoch 70, Loss: 0.5663739209081612\n",
            "Dataset 13 model training, Epoch 71, Loss: 0.5710293839569964\n",
            "Dataset 13 model training, Epoch 72, Loss: 0.5688576363270579\n",
            "Dataset 13 model training, Epoch 73, Loss: 0.5711985481720344\n",
            "Dataset 13 model training, Epoch 74, Loss: 0.5653278605610716\n",
            "Dataset 13 model training, Epoch 75, Loss: 0.5725898534254311\n",
            "Dataset 13 model training, Epoch 76, Loss: 0.5629546911108727\n",
            "Dataset 13 model training, Epoch 77, Loss: 0.5660454404899498\n",
            "Dataset 13 model training, Epoch 78, Loss: 0.5724310458096025\n",
            "Dataset 13 model training, Epoch 79, Loss: 0.5649262000532711\n",
            "Dataset 13 model training, Epoch 80, Loss: 0.5657792981543572\n",
            "Dataset 13 model training, Epoch 81, Loss: 0.5690937131838082\n",
            "Dataset 13 model training, Epoch 82, Loss: 0.5588098667026346\n",
            "Dataset 13 model training, Epoch 83, Loss: 0.5585682982323217\n",
            "Dataset 13 model training, Epoch 84, Loss: 0.5555497540367974\n",
            "Dataset 13 model training, Epoch 85, Loss: 0.5633370490635142\n",
            "Dataset 13 model training, Epoch 86, Loss: 0.5558809947733786\n",
            "Dataset 13 model training, Epoch 87, Loss: 0.5652763005954767\n",
            "Dataset 13 model training, Epoch 88, Loss: 0.5613269946154427\n",
            "Dataset 13 model training, Epoch 89, Loss: 0.5587845684266558\n",
            "Dataset 13 model training, Epoch 90, Loss: 0.5606309251458037\n",
            "Dataset 13 model training, Epoch 91, Loss: 0.5548913052269057\n",
            "Dataset 13 model training, Epoch 92, Loss: 0.5509603013789731\n",
            "Dataset 13 model training, Epoch 93, Loss: 0.5478734348724091\n",
            "Dataset 13 model training, Epoch 94, Loss: 0.5494797368065204\n",
            "Dataset 13 model training, Epoch 95, Loss: 0.5481382719441956\n",
            "Dataset 13 model training, Epoch 96, Loss: 0.5531935162014432\n",
            "Dataset 13 model training, Epoch 97, Loss: 0.5518332412040311\n",
            "Dataset 13 model training, Epoch 98, Loss: 0.5486934905737834\n",
            "Dataset 13 model training, Epoch 99, Loss: 0.5549360879885605\n",
            "Dataset 13 model training, Epoch 100, Loss: 0.5372979467600779\n",
            "Dataset 13 model training, Epoch 101, Loss: 0.5384349733396293\n",
            "Dataset 13 model training, Epoch 102, Loss: 0.543457862988017\n",
            "Dataset 13 model training, Epoch 103, Loss: 0.5359682976420409\n",
            "Dataset 13 model training, Epoch 104, Loss: 0.536832112307642\n",
            "Dataset 13 model training, Epoch 105, Loss: 0.5453850217894012\n",
            "Dataset 13 model training, Epoch 106, Loss: 0.5427570734538284\n",
            "Dataset 13 model training, Epoch 107, Loss: 0.5317216570081275\n",
            "Dataset 13 model training, Epoch 108, Loss: 0.5405550628316169\n",
            "Dataset 13 model training, Epoch 109, Loss: 0.535741566053403\n",
            "Dataset 13 model training, Epoch 110, Loss: 0.5406696671753927\n",
            "Dataset 13 model training, Epoch 111, Loss: 0.5391084602455688\n",
            "Dataset 13 model training, Epoch 112, Loss: 0.5316471212050494\n",
            "Dataset 13 model training, Epoch 113, Loss: 0.5374490030450758\n",
            "Dataset 13 model training, Epoch 114, Loss: 0.5470298267657461\n",
            "Dataset 13 model training, Epoch 115, Loss: 0.5423945940397923\n",
            "Dataset 13 model training, Epoch 116, Loss: 0.5365296218519896\n",
            "Dataset 13 model training, Epoch 117, Loss: 0.5413069723088757\n",
            "Dataset 13 model training, Epoch 118, Loss: 0.5339343676380083\n",
            "Dataset 13 model training, Epoch 119, Loss: 0.5428940353829876\n",
            "Dataset 13 model training, Epoch 120, Loss: 0.5383973037884906\n",
            "Dataset 13 model training, Epoch 121, Loss: 0.5359882118265613\n",
            "Dataset 13 model training, Epoch 122, Loss: 0.53081971387458\n",
            "Dataset 13 model training, Epoch 123, Loss: 0.5298737517369339\n",
            "Dataset 13 model training, Epoch 124, Loss: 0.5363347723203546\n",
            "Dataset 13 model training, Epoch 125, Loss: 0.5325738199396071\n",
            "Dataset 13 model training, Epoch 126, Loss: 0.534652949937808\n",
            "Dataset 13 model training, Epoch 127, Loss: 0.530064755015903\n",
            "Dataset 13 model training, Epoch 128, Loss: 0.5377940549180399\n",
            "Dataset 13 model training, Epoch 129, Loss: 0.5343882425937777\n",
            "Dataset 13 model training, Epoch 130, Loss: 0.5277964155268825\n",
            "Dataset 13 model training, Epoch 131, Loss: 0.5358146547881606\n",
            "Dataset 13 model training, Epoch 132, Loss: 0.5242933243318321\n",
            "Dataset 13 model training, Epoch 133, Loss: 0.5250567811376908\n",
            "Dataset 13 model training, Epoch 134, Loss: 0.5309165733702043\n",
            "Dataset 13 model training, Epoch 135, Loss: 0.5271515106064042\n",
            "Dataset 13 model training, Epoch 136, Loss: 0.5274446135252909\n",
            "Dataset 13 model training, Epoch 137, Loss: 0.5254245229016722\n",
            "Dataset 13 model training, Epoch 138, Loss: 0.5316122702134201\n",
            "Dataset 13 model training, Epoch 139, Loss: 0.5227113299899631\n",
            "Dataset 13 model training, Epoch 140, Loss: 0.5344488782430786\n",
            "Dataset 13 model training, Epoch 141, Loss: 0.529565105056451\n",
            "Dataset 13 model training, Epoch 142, Loss: 0.5267083605909659\n",
            "Dataset 13 model training, Epoch 143, Loss: 0.5282688976502886\n",
            "Dataset 13 model training, Epoch 144, Loss: 0.5219031210039177\n",
            "Dataset 13 model training, Epoch 145, Loss: 0.5261121306544035\n",
            "Dataset 13 model training, Epoch 146, Loss: 0.5299288275974249\n",
            "Dataset 13 model training, Epoch 147, Loss: 0.5265238555817823\n",
            "Dataset 13 model training, Epoch 148, Loss: 0.5259913957976048\n",
            "Dataset 13 model training, Epoch 149, Loss: 0.5285844946998397\n",
            "Dataset 13 model training, Epoch 150, Loss: 0.5342777457112581\n",
            "Dataset 13 model training, Epoch 151, Loss: 0.5307488585609237\n",
            "Dataset 13 model training, Epoch 152, Loss: 0.5276854963474025\n",
            "Dataset 13 model training, Epoch 153, Loss: 0.5274326900641123\n",
            "Early stopping triggered for Dataset 13 model after epoch 154\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 14/15\n",
            "Dataset 14 model training, Epoch 1, Loss: 1.4365189478097373\n",
            "Dataset 14 model training, Epoch 2, Loss: 1.1617583944308048\n",
            "Dataset 14 model training, Epoch 3, Loss: 1.0073259468899658\n",
            "Dataset 14 model training, Epoch 4, Loss: 0.9248605575782574\n",
            "Dataset 14 model training, Epoch 5, Loss: 0.880012231946781\n",
            "Dataset 14 model training, Epoch 6, Loss: 0.8511511067680965\n",
            "Dataset 14 model training, Epoch 7, Loss: 0.8214530625090694\n",
            "Dataset 14 model training, Epoch 8, Loss: 0.8087510790256475\n",
            "Dataset 14 model training, Epoch 9, Loss: 0.7882638530225943\n",
            "Dataset 14 model training, Epoch 10, Loss: 0.7818549555658505\n",
            "Dataset 14 model training, Epoch 11, Loss: 0.7653452927703099\n",
            "Dataset 14 model training, Epoch 12, Loss: 0.7617670798933269\n",
            "Dataset 14 model training, Epoch 13, Loss: 0.7345283438038352\n",
            "Dataset 14 model training, Epoch 14, Loss: 0.7209217185216235\n",
            "Dataset 14 model training, Epoch 15, Loss: 0.708076168764506\n",
            "Dataset 14 model training, Epoch 16, Loss: 0.6966524195197402\n",
            "Dataset 14 model training, Epoch 17, Loss: 0.6769987897367666\n",
            "Dataset 14 model training, Epoch 18, Loss: 0.6787694149854168\n",
            "Dataset 14 model training, Epoch 19, Loss: 0.6693775855152813\n",
            "Dataset 14 model training, Epoch 20, Loss: 0.6642358745170744\n",
            "Dataset 14 model training, Epoch 21, Loss: 0.657847207113607\n",
            "Dataset 14 model training, Epoch 22, Loss: 0.6533776024319479\n",
            "Dataset 14 model training, Epoch 23, Loss: 0.6422874145949913\n",
            "Dataset 14 model training, Epoch 24, Loss: 0.6392434870959908\n",
            "Dataset 14 model training, Epoch 25, Loss: 0.6420772861171242\n",
            "Dataset 14 model training, Epoch 26, Loss: 0.6375808358587176\n",
            "Dataset 14 model training, Epoch 27, Loss: 0.6298264134798618\n",
            "Dataset 14 model training, Epoch 28, Loss: 0.6332735119276489\n",
            "Dataset 14 model training, Epoch 29, Loss: 0.638003013583998\n",
            "Dataset 14 model training, Epoch 30, Loss: 0.6341077292597057\n",
            "Dataset 14 model training, Epoch 31, Loss: 0.6331018659452728\n",
            "Dataset 14 model training, Epoch 32, Loss: 0.613489243763172\n",
            "Dataset 14 model training, Epoch 33, Loss: 0.6125296257584301\n",
            "Dataset 14 model training, Epoch 34, Loss: 0.6253774511498331\n",
            "Dataset 14 model training, Epoch 35, Loss: 0.6242176793663707\n",
            "Dataset 14 model training, Epoch 36, Loss: 0.6234302879958753\n",
            "Dataset 14 model training, Epoch 37, Loss: 0.617750661657346\n",
            "Dataset 14 model training, Epoch 38, Loss: 0.6178597178285485\n",
            "Dataset 14 model training, Epoch 39, Loss: 0.6212394634224722\n",
            "Dataset 14 model training, Epoch 40, Loss: 0.6099747636460311\n",
            "Dataset 14 model training, Epoch 41, Loss: 0.5985041092957882\n",
            "Dataset 14 model training, Epoch 42, Loss: 0.5983397729744185\n",
            "Dataset 14 model training, Epoch 43, Loss: 0.5885142026752825\n",
            "Dataset 14 model training, Epoch 44, Loss: 0.5880110925001814\n",
            "Dataset 14 model training, Epoch 45, Loss: 0.5967061594622025\n",
            "Dataset 14 model training, Epoch 46, Loss: 0.5938894314481723\n",
            "Dataset 14 model training, Epoch 47, Loss: 0.5878784214423982\n",
            "Dataset 14 model training, Epoch 48, Loss: 0.5819885939951764\n",
            "Dataset 14 model training, Epoch 49, Loss: 0.5870121087854272\n",
            "Dataset 14 model training, Epoch 50, Loss: 0.5853036349972353\n",
            "Dataset 14 model training, Epoch 51, Loss: 0.5897232248688375\n",
            "Dataset 14 model training, Epoch 52, Loss: 0.5800834225108292\n",
            "Dataset 14 model training, Epoch 53, Loss: 0.5835152953271044\n",
            "Dataset 14 model training, Epoch 54, Loss: 0.5824814067771104\n",
            "Dataset 14 model training, Epoch 55, Loss: 0.5842643749240218\n",
            "Dataset 14 model training, Epoch 56, Loss: 0.5892547623605917\n",
            "Dataset 14 model training, Epoch 57, Loss: 0.5859467927983265\n",
            "Dataset 14 model training, Epoch 58, Loss: 0.585202086642878\n",
            "Dataset 14 model training, Epoch 59, Loss: 0.5746518618223683\n",
            "Dataset 14 model training, Epoch 60, Loss: 0.5630276094604012\n",
            "Dataset 14 model training, Epoch 61, Loss: 0.5628485723047068\n",
            "Dataset 14 model training, Epoch 62, Loss: 0.5670074109999549\n",
            "Dataset 14 model training, Epoch 63, Loss: 0.5686279047798637\n",
            "Dataset 14 model training, Epoch 64, Loss: 0.5627964644242596\n",
            "Dataset 14 model training, Epoch 65, Loss: 0.5662559055729418\n",
            "Dataset 14 model training, Epoch 66, Loss: 0.5654792588278158\n",
            "Dataset 14 model training, Epoch 67, Loss: 0.562786569066395\n",
            "Dataset 14 model training, Epoch 68, Loss: 0.5626051536459007\n",
            "Dataset 14 model training, Epoch 69, Loss: 0.5557946211454884\n",
            "Dataset 14 model training, Epoch 70, Loss: 0.5587100151753583\n",
            "Dataset 14 model training, Epoch 71, Loss: 0.5677116122466839\n",
            "Dataset 14 model training, Epoch 72, Loss: 0.5562580825871979\n",
            "Dataset 14 model training, Epoch 73, Loss: 0.5595398880788033\n",
            "Dataset 14 model training, Epoch 74, Loss: 0.5476171178928274\n",
            "Dataset 14 model training, Epoch 75, Loss: 0.5640116545143506\n",
            "Dataset 14 model training, Epoch 76, Loss: 0.5578273278198495\n",
            "Dataset 14 model training, Epoch 77, Loss: 0.5553104599185338\n",
            "Dataset 14 model training, Epoch 78, Loss: 0.5597916551378389\n",
            "Dataset 14 model training, Epoch 79, Loss: 0.5517666979341318\n",
            "Dataset 14 model training, Epoch 80, Loss: 0.554760361941445\n",
            "Dataset 14 model training, Epoch 81, Loss: 0.5490996930378162\n",
            "Dataset 14 model training, Epoch 82, Loss: 0.5358968181720632\n",
            "Dataset 14 model training, Epoch 83, Loss: 0.5503992882390686\n",
            "Dataset 14 model training, Epoch 84, Loss: 0.548469585101336\n",
            "Dataset 14 model training, Epoch 85, Loss: 0.5450308342643132\n",
            "Dataset 14 model training, Epoch 86, Loss: 0.5410394637000482\n",
            "Dataset 14 model training, Epoch 87, Loss: 0.5410250703625331\n",
            "Dataset 14 model training, Epoch 88, Loss: 0.545269280474707\n",
            "Dataset 14 model training, Epoch 89, Loss: 0.535340154605196\n",
            "Dataset 14 model training, Epoch 90, Loss: 0.5351771005731545\n",
            "Dataset 14 model training, Epoch 91, Loss: 0.5360710918903351\n",
            "Dataset 14 model training, Epoch 92, Loss: 0.5427745229361073\n",
            "Dataset 14 model training, Epoch 93, Loss: 0.5374609829968964\n",
            "Dataset 14 model training, Epoch 94, Loss: 0.5403262874148539\n",
            "Dataset 14 model training, Epoch 95, Loss: 0.5415690999157381\n",
            "Dataset 14 model training, Epoch 96, Loss: 0.5387618403561067\n",
            "Dataset 14 model training, Epoch 97, Loss: 0.5395073645951732\n",
            "Dataset 14 model training, Epoch 98, Loss: 0.5338821580867894\n",
            "Dataset 14 model training, Epoch 99, Loss: 0.5305625146982685\n",
            "Dataset 14 model training, Epoch 100, Loss: 0.5370002173824816\n",
            "Dataset 14 model training, Epoch 101, Loss: 0.5305175603620264\n",
            "Dataset 14 model training, Epoch 102, Loss: 0.5312055556584667\n",
            "Dataset 14 model training, Epoch 103, Loss: 0.529043032238815\n",
            "Dataset 14 model training, Epoch 104, Loss: 0.5306588190280839\n",
            "Dataset 14 model training, Epoch 105, Loss: 0.5300313234329224\n",
            "Dataset 14 model training, Epoch 106, Loss: 0.5243588776777912\n",
            "Dataset 14 model training, Epoch 107, Loss: 0.5322570173156183\n",
            "Dataset 14 model training, Epoch 108, Loss: 0.5335081466381123\n",
            "Dataset 14 model training, Epoch 109, Loss: 0.5275878491780616\n",
            "Dataset 14 model training, Epoch 110, Loss: 0.5332480657179623\n",
            "Dataset 14 model training, Epoch 111, Loss: 0.5337174057171045\n",
            "Dataset 14 model training, Epoch 112, Loss: 0.5253213961787572\n",
            "Dataset 14 model training, Epoch 113, Loss: 0.5282284007561917\n",
            "Dataset 14 model training, Epoch 114, Loss: 0.5344436206170265\n",
            "Dataset 14 model training, Epoch 115, Loss: 0.5296644102658657\n",
            "Early stopping triggered for Dataset 14 model after epoch 116\n",
            "--------------------------------------------------\n",
            "Training model for Dataset 15/15\n",
            "Dataset 15 model training, Epoch 1, Loss: 1.4062074349351126\n",
            "Dataset 15 model training, Epoch 2, Loss: 1.1411355493003374\n",
            "Dataset 15 model training, Epoch 3, Loss: 0.9932884349398416\n",
            "Dataset 15 model training, Epoch 4, Loss: 0.890172588090374\n",
            "Dataset 15 model training, Epoch 5, Loss: 0.832372689083831\n",
            "Dataset 15 model training, Epoch 6, Loss: 0.8126492843236008\n",
            "Dataset 15 model training, Epoch 7, Loss: 0.7728072539584278\n",
            "Dataset 15 model training, Epoch 8, Loss: 0.7526748943818758\n",
            "Dataset 15 model training, Epoch 9, Loss: 0.7503268641151793\n",
            "Dataset 15 model training, Epoch 10, Loss: 0.7331370592933811\n",
            "Dataset 15 model training, Epoch 11, Loss: 0.7258412588949072\n",
            "Dataset 15 model training, Epoch 12, Loss: 0.711865487980516\n",
            "Dataset 15 model training, Epoch 13, Loss: 0.7038924816536577\n",
            "Dataset 15 model training, Epoch 14, Loss: 0.6876746011923437\n",
            "Dataset 15 model training, Epoch 15, Loss: 0.6810849494721791\n",
            "Dataset 15 model training, Epoch 16, Loss: 0.6631652806719689\n",
            "Dataset 15 model training, Epoch 17, Loss: 0.6579343676567078\n",
            "Dataset 15 model training, Epoch 18, Loss: 0.6452310187359379\n",
            "Dataset 15 model training, Epoch 19, Loss: 0.641069281182877\n",
            "Dataset 15 model training, Epoch 20, Loss: 0.6243632650130415\n",
            "Dataset 15 model training, Epoch 21, Loss: 0.6281408645110588\n",
            "Dataset 15 model training, Epoch 22, Loss: 0.6120560789761478\n",
            "Dataset 15 model training, Epoch 23, Loss: 0.6141445700028171\n",
            "Dataset 15 model training, Epoch 24, Loss: 0.6190712333542027\n",
            "Dataset 15 model training, Epoch 25, Loss: 0.6061881145794098\n",
            "Dataset 15 model training, Epoch 26, Loss: 0.6095683468531256\n",
            "Dataset 15 model training, Epoch 27, Loss: 0.6064713674865357\n",
            "Dataset 15 model training, Epoch 28, Loss: 0.5977158648510502\n",
            "Dataset 15 model training, Epoch 29, Loss: 0.600852932219636\n",
            "Dataset 15 model training, Epoch 30, Loss: 0.5930171019002183\n",
            "Dataset 15 model training, Epoch 31, Loss: 0.5929405134018153\n",
            "Dataset 15 model training, Epoch 32, Loss: 0.5890145507985598\n",
            "Dataset 15 model training, Epoch 33, Loss: 0.5855735854743278\n",
            "Dataset 15 model training, Epoch 34, Loss: 0.5887948323602545\n",
            "Dataset 15 model training, Epoch 35, Loss: 0.5791594039087427\n",
            "Dataset 15 model training, Epoch 36, Loss: 0.5861775856720258\n",
            "Dataset 15 model training, Epoch 37, Loss: 0.5838364309644046\n",
            "Dataset 15 model training, Epoch 38, Loss: 0.5827705159987489\n",
            "Dataset 15 model training, Epoch 39, Loss: 0.5768231818937275\n",
            "Dataset 15 model training, Epoch 40, Loss: 0.576574056115869\n",
            "Dataset 15 model training, Epoch 41, Loss: 0.5827752155800389\n",
            "Dataset 15 model training, Epoch 42, Loss: 0.5858172858006334\n",
            "Dataset 15 model training, Epoch 43, Loss: 0.577367463748749\n",
            "Dataset 15 model training, Epoch 44, Loss: 0.5678447299215892\n",
            "Dataset 15 model training, Epoch 45, Loss: 0.5740729709194131\n",
            "Dataset 15 model training, Epoch 46, Loss: 0.5819848937122789\n",
            "Dataset 15 model training, Epoch 47, Loss: 0.5704426391892237\n",
            "Dataset 15 model training, Epoch 48, Loss: 0.5739349504447964\n",
            "Dataset 15 model training, Epoch 49, Loss: 0.5744050592184067\n",
            "Dataset 15 model training, Epoch 50, Loss: 0.576269464133537\n",
            "Dataset 15 model training, Epoch 51, Loss: 0.5636997090218818\n",
            "Dataset 15 model training, Epoch 52, Loss: 0.5591399222612381\n",
            "Dataset 15 model training, Epoch 53, Loss: 0.5616224236684303\n",
            "Dataset 15 model training, Epoch 54, Loss: 0.5523232972785218\n",
            "Dataset 15 model training, Epoch 55, Loss: 0.550254809938065\n",
            "Dataset 15 model training, Epoch 56, Loss: 0.5564762496784942\n",
            "Dataset 15 model training, Epoch 57, Loss: 0.5448023497241817\n",
            "Dataset 15 model training, Epoch 58, Loss: 0.550791226021231\n",
            "Dataset 15 model training, Epoch 59, Loss: 0.5464588103637303\n",
            "Dataset 15 model training, Epoch 60, Loss: 0.5521435286492518\n",
            "Dataset 15 model training, Epoch 61, Loss: 0.5463938533443294\n",
            "Dataset 15 model training, Epoch 62, Loss: 0.540129131037895\n",
            "Dataset 15 model training, Epoch 63, Loss: 0.5525001409935625\n",
            "Dataset 15 model training, Epoch 64, Loss: 0.5421219598757078\n",
            "Dataset 15 model training, Epoch 65, Loss: 0.5420861072736244\n",
            "Dataset 15 model training, Epoch 66, Loss: 0.545332719201911\n",
            "Dataset 15 model training, Epoch 67, Loss: 0.5388028368557969\n",
            "Dataset 15 model training, Epoch 68, Loss: 0.5418711214849393\n",
            "Dataset 15 model training, Epoch 69, Loss: 0.5417385515693116\n",
            "Dataset 15 model training, Epoch 70, Loss: 0.5482443950764121\n",
            "Dataset 15 model training, Epoch 71, Loss: 0.5438521110439953\n",
            "Dataset 15 model training, Epoch 72, Loss: 0.5415448932206794\n",
            "Dataset 15 model training, Epoch 73, Loss: 0.541626852260877\n",
            "Dataset 15 model training, Epoch 74, Loss: 0.5374460150934246\n",
            "Dataset 15 model training, Epoch 75, Loss: 0.5284040933602476\n",
            "Dataset 15 model training, Epoch 76, Loss: 0.5286824294965561\n",
            "Dataset 15 model training, Epoch 77, Loss: 0.5205157148103191\n",
            "Dataset 15 model training, Epoch 78, Loss: 0.5221985852065152\n",
            "Dataset 15 model training, Epoch 79, Loss: 0.5181381294172104\n",
            "Dataset 15 model training, Epoch 80, Loss: 0.5174741271423967\n",
            "Dataset 15 model training, Epoch 81, Loss: 0.509839459233088\n",
            "Dataset 15 model training, Epoch 82, Loss: 0.5160278720806722\n",
            "Dataset 15 model training, Epoch 83, Loss: 0.5147080225487278\n",
            "Dataset 15 model training, Epoch 84, Loss: 0.5228645891359408\n",
            "Dataset 15 model training, Epoch 85, Loss: 0.5215050769995336\n",
            "Dataset 15 model training, Epoch 86, Loss: 0.5187522775098069\n",
            "Dataset 15 model training, Epoch 87, Loss: 0.5191077714505261\n",
            "Dataset 15 model training, Epoch 88, Loss: 0.5158047292330493\n",
            "Dataset 15 model training, Epoch 89, Loss: 0.510160649475986\n",
            "Dataset 15 model training, Epoch 90, Loss: 0.4977564989295724\n",
            "Dataset 15 model training, Epoch 91, Loss: 0.5048089462192091\n",
            "Dataset 15 model training, Epoch 92, Loss: 0.5060670602403275\n",
            "Dataset 15 model training, Epoch 93, Loss: 0.5099171259223598\n",
            "Dataset 15 model training, Epoch 94, Loss: 0.5056517671232355\n",
            "Dataset 15 model training, Epoch 95, Loss: 0.49496098883347966\n",
            "Dataset 15 model training, Epoch 96, Loss: 0.5068328668401666\n",
            "Dataset 15 model training, Epoch 97, Loss: 0.5013146014654473\n",
            "Dataset 15 model training, Epoch 98, Loss: 0.50163658412352\n",
            "Dataset 15 model training, Epoch 99, Loss: 0.49273827549529403\n",
            "Dataset 15 model training, Epoch 100, Loss: 0.4920937135203244\n",
            "Dataset 15 model training, Epoch 101, Loss: 0.4935337501029446\n",
            "Dataset 15 model training, Epoch 102, Loss: 0.496592493906413\n",
            "Dataset 15 model training, Epoch 103, Loss: 0.49524028395136743\n",
            "Dataset 15 model training, Epoch 104, Loss: 0.49343129461758756\n",
            "Dataset 15 model training, Epoch 105, Loss: 0.5055928128222896\n",
            "Dataset 15 model training, Epoch 106, Loss: 0.49353287420044206\n",
            "Dataset 15 model training, Epoch 107, Loss: 0.4975528057715664\n",
            "Dataset 15 model training, Epoch 108, Loss: 0.4897880394981332\n",
            "Dataset 15 model training, Epoch 109, Loss: 0.48758676627727404\n",
            "Dataset 15 model training, Epoch 110, Loss: 0.49725418997137516\n",
            "Dataset 15 model training, Epoch 111, Loss: 0.48271267585558436\n",
            "Dataset 15 model training, Epoch 112, Loss: 0.4824454180181843\n",
            "Dataset 15 model training, Epoch 113, Loss: 0.49253551008766644\n",
            "Dataset 15 model training, Epoch 114, Loss: 0.4843514510621763\n",
            "Dataset 15 model training, Epoch 115, Loss: 0.4836869576614197\n",
            "Dataset 15 model training, Epoch 116, Loss: 0.4859444323467882\n",
            "Dataset 15 model training, Epoch 117, Loss: 0.4866959750652313\n",
            "Dataset 15 model training, Epoch 118, Loss: 0.4829166304983505\n",
            "Dataset 15 model training, Epoch 119, Loss: 0.48767784115386337\n",
            "Dataset 15 model training, Epoch 120, Loss: 0.48149199130600445\n",
            "Dataset 15 model training, Epoch 121, Loss: 0.48009975274948224\n",
            "Dataset 15 model training, Epoch 122, Loss: 0.48102958867811174\n",
            "Dataset 15 model training, Epoch 123, Loss: 0.4815513883551506\n",
            "Dataset 15 model training, Epoch 124, Loss: 0.4701605786199439\n",
            "Dataset 15 model training, Epoch 125, Loss: 0.48509770835915655\n",
            "Dataset 15 model training, Epoch 126, Loss: 0.47920854777505956\n",
            "Dataset 15 model training, Epoch 127, Loss: 0.4782532020790936\n",
            "Dataset 15 model training, Epoch 128, Loss: 0.4814068815887791\n",
            "Dataset 15 model training, Epoch 129, Loss: 0.48299505673859217\n",
            "Dataset 15 model training, Epoch 130, Loss: 0.4805454176990953\n",
            "Dataset 15 model training, Epoch 131, Loss: 0.4778723604466817\n",
            "Dataset 15 model training, Epoch 132, Loss: 0.47631868352628737\n",
            "Dataset 15 model training, Epoch 133, Loss: 0.48378663520290427\n",
            "Early stopping triggered for Dataset 15 model after epoch 134\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the aggregated model\n",
        "evaluate(global_model, test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYg9CEoGR01R",
        "outputId": "2bc3f7ba-c041-49ef-9c3a-4af04795a022"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: Accuracy: 70.90530697190427%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}